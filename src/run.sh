# GPT2 CLM
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/GPT2.0.1 --tokenizer_name gpt2-xl --output_dir gpt-0.1 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/GPT2.0.5 --tokenizer_name gpt2-xl --output_dir gpt-0.5 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/GPT2.0.9 --tokenizer_name gpt2-xl --output_dir gpt-0.9 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/GPT2.0.95 --tokenizer_name gpt2-xl --output_dir gpt-0.95 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/GPT2.0.99 --tokenizer_name gpt2-xl --output_dir gpt-0.99 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path gpt2-xl --tokenizer_name gpt2-xl --output_dir gpt2-xl --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir

# BART CLM
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/bart.0.99 --tokenizer_name facebook/bart-large --output_dir bart-0.99 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/bart.0.95 --tokenizer_name facebook/bart-large --output_dir bart-0.95 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/bart.0.9 --tokenizer_name facebook/bart-large --output_dir bart-0.9 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/bart.0.5 --tokenizer_name facebook/bart-large --output_dir bart-0.5 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/bart.0.1 --tokenizer_name facebook/bart-large --output_dir bart-0.1 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path facebook/bart-large --tokenizer_name facebook/bart-large --output_dir bart-base --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir

# BERT CLM
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/bert.0.1 --tokenizer_name gpt2-xl --output_dir gpt-0.1 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/bert.0.5 --tokenizer_name gpt2-xl --output_dir gpt-0.5 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/bert.0.9 --tokenizer_name gpt2-xl --output_dir gpt-0.9 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/bert.0.95 --tokenizer_name gpt2-xl --output_dir gpt-0.95 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path ~/sparse/bert.0.99 --tokenizer_name gpt2-xl --output_dir gpt-0.99 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir
torchrun pytorch/language-modeling/run_clm.py --model_name_or_path bert-large-cased --tokenizer_name bert-large-cased --output_dir bert-base --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --num_train_epochs 1 --overwrite_output_dir

## GPT2 XNLI
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/gpt2_glue.0.99 --tokenizer_name ~/sparse/gpt2_glue/ --output_dir gpt_0.99_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/gpt2_glue.0.95 --tokenizer_name ~/sparse/gpt2_glue/ --output_dir gpt_0.95_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/gpt2_glue.0.9 --tokenizer_name ~/sparse/gpt2_glue/ --output_dir gpt_0.9_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/gpt2_glue.0.5 --tokenizer_name ~/sparse/gpt2_glue/ --output_dir gpt_0.5_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/gpt2_glue.0.1 --tokenizer_name ~/sparse/gpt2_glue/ --output_dir gpt_0.1_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/gpt2_glue --tokenizer_name ~/sparse/gpt2_glue/ --output_dir gpt_base_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1

## BART XNLI
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/bart.0.99 --tokenizer_name facebook/bart-large --output_dir bart_0.99_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/bart.0.95 --tokenizer_name facebook/bart-large --output_dir bart_0.95_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/bart.0.9 --tokenizer_name facebook/bart-large --output_dir bart_0.9_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/bart.0.5 --tokenizer_name facebook/bart-large --output_dir bart_0.5_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/bart.0.1 --tokenizer_name facebook/bart-large --output_dir bart_0.1_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path facebook/bart-large --tokenizer_name facebook/bart-large --output_dir bart_base_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1

## BERT XNLI
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/bert.0.99 --tokenizer_name bert-large-cased --output_dir bert_0.99_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/bert.0.95 --tokenizer_name bert-large-cased --output_dir bert_0.95_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/bert.0.9 --tokenizer_name bert-large-cased --output_dir bert_0.9_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/bert.0.5 --tokenizer_name bert-large-cased --output_dir bert_0.5_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path ~/sparse/bert.0.1 --tokenizer_name bert-large-cased --output_dir bert_0.1_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
torchrun pytorch/text-classification/run_xnli.py --model_name_or_path bert-large-cased --tokenizer_name bert-large-cased --output_dir bert_base_xnli --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --do_eval  --language en --num_train_epochs 0.1
