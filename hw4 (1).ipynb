{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopherkang/.conda/envs/llm/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoModel\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2Model, DebertaV2ForQuestionAnswering\n",
    "\n",
    "from datasets import load_dataset\n",
    "from evaluate import evaluator\n",
    "from transformers import AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration j0hngou--ccmatrix_de-en-1f42eeb206fbff2d\n",
      "Found cached dataset parquet (/home/christopherkang/.cache/huggingface/datasets/j0hngou___parquet/j0hngou--ccmatrix_de-en-1f42eeb206fbff2d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 594.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Er wurde 2004 unter strengen Auflagen freigelassen.', 'Gibt es eine ideale Form?', 'Sie sind das Hauptwerk des \"Naumburger Meisters\", eines namentlich unbekannten, wohl aus Frankreich stammenden und dort ausgebildeten Steinbildhauers und Architekten und seines Werktrupps.', '„Ich werde dir nun zum Ausgleich für alles, was du mir gesagt hast, ein paar Informationen geben.', 'Sasha setzt sich sehr dafür ein, dass nicht-binäre Geschlechter anerkannt werden, und hatte sogar eine Online-Petition gestartet, um die Aufmerksamkeit von Präsident Obama auf das Thema zu lenken.', 'Geschäftsagilität wird durch das einfache, modulare Design und eine Vielzahl nützlicher Merkmale sichergestellt, die dazu beitragen, die Bereitstellungszeit auf wenige Stunden oder sogar nur Minuten zu reduzieren.', 'Die Damen entscheiden, mit wem sie sich paaren.', '1) Die Eigentümerstruktur japanischer Unternehmen hat sich von einem geschlossenen, Insider-basierten System zu einem offenen und wettbewerbsorientierten System gewandelt', '....nochmals ein ganz großes Dankeschön für den wunderbaren Heilungsabend.', 'Sie war ein Lebensretter von der get go.']\n"
     ]
    }
   ],
   "source": [
    "# test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "# test = load_dataset(\"yhavinga/ccmatrix\", \"de-en\", streaming=True)[\"train\"].take(10)\n",
    "test = load_dataset(\"j0hngou/ccmatrix_de-en\")\n",
    "print([x[\"de\"] for x in test[\"train\"][\"translation\"]][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2-XL\n",
    "\n",
    "[Perplexity](https://huggingface.co/docs/transformers/perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/christopherkang/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (331586 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████▉| 646/648 [01:00<00:00, 10.69it/s]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "device = \"cuda\"\n",
    "# model_id = \"gpt2-large\"\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\").to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\").to(device)\n",
    "\n",
    "# parameters_to_prune = [(m, \"weight\") for m in filter(lambda m: hasattr(m, \"weight\"), model.modules())]\n",
    "# parameters_to_prune = [(m[1], \"weight\") for m in filter(lambda nm: hasattr(nm[1], \"weight\") and \"mlp.c_fc\" in nm[0], model.named_modules())]\n",
    "# prune.global_unstructured(\n",
    "#     parameters_to_prune,\n",
    "#     pruning_method=prune.L1Unstructured,\n",
    "#     amount=0.95,\n",
    "# )\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# max_length = model.config.n_positions\n",
    "max_lengh = 1024\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over input tokens.\n",
    "        # Multiply it with trg_len to get the summation instead of average.\n",
    "        # We will take average over all the tokens to get the true average\n",
    "        # in the last step of this example.\n",
    "        neg_log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8096.9810, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPT2_XL_results = [14.7877, 14.7825, 36.9467, 4670.9092, OOM, OOM, OOM]\n",
    "## FB_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluating M2M-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True),\n",
       " Linear(in_features=1024, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=1024, bias=True)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_to_prune = [m[1] for m in model.named_modules() if \"fc\" in m[0]]\n",
    "parameters_to_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = [m for m in model.named_modules() if \"fc\" in m[0] and m[1].out_features==8192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration j0hngou--ccmatrix_de-en-1f42eeb206fbff2d\n",
      "Found cached dataset parquet (/home/christopherkang/.cache/huggingface/datasets/j0hngou___parquet/j0hngou--ccmatrix_de-en-1f42eeb206fbff2d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]/home/christopherkang/.conda/envs/llm/lib/python3.7/site-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n",
      "  0%|          | 1/300 [00:01<06:06,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G G G G G']\n",
      "He was released in 2006 under stringent conditions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 12/300 [00:03<00:50,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I I']\n",
      "Let’s look at both options and why going through a company is currently the best way to extract Bitcoins for profit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 22/300 [00:04<00:35,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I']\n",
      "I’m familiar with birds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 32/300 [00:05<00:43,  6.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I I I I I']\n",
      "Of course, planning is thinking through how to get where you want to go and taking precautions to ensure you don’t wind up where you don’t want to go.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 42/300 [00:07<00:40,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I']\n",
      "39 At the end of the two months, she came back to her father.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 52/300 [00:09<00:41,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I']\n",
      "“So my doctors want me to continue to rest my voice.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 62/300 [00:10<00:34,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C C C C C C C C']\n",
      "This particular dimension creates this element much more intensely than any other dimension physically focused.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 72/300 [00:12<00:35,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I']\n",
      "The PAL photo ID card is valid for five years and shows the machine categories in which the operator has been trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 82/300 [00:13<00:30,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "there are empty bottles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 92/300 [00:15<00:34,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I I']\n",
      "Christopher Nolan seems incapable of directing a bad film.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 102/300 [00:17<00:27,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I']\n",
      "With 2012 well on its way, many business owners are looking to maximize their business potential this year.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 112/300 [00:18<00:32,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I I']\n",
      "Determination of a threat to the peace breach of the peace or act of aggression under Article 39 of the Charter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 122/300 [00:20<00:30,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I']\n",
      "The infoBoard web application \"HR Permissions\" gives you an overview of all vacation requests and all requests for absenteeism of the employees in your company, which were previously submitted in \"IB HRRequests\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 132/300 [00:22<00:30,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I I']\n",
      "Her attitude towards me was far from the best as she admired the previous President and she was on his side.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 142/300 [00:24<00:23,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I']\n",
      "“The long-term vision is, once we’ve been able to analyze all the data from CAMP, we would have a series of panels,” said David G. Amaral, one of the researchers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 152/300 [00:25<00:24,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I']\n",
      "No one learns faster than those who teach.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 161/300 [00:27<00:20,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I']\n",
      "The data protection guidelines are an integral part of these terms of use and regulate the collection, storage and use of your personal data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 171/300 [00:28<00:18,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I']\n",
      "This is a taste of what I mean by \"tortured\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 181/300 [00:30<00:21,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I']\n",
      "But I can absolutely tell you, in the NSA world defense wins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 192/300 [00:32<00:17,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I']\n",
      "Mairo was there, washing her hair in a lavatory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 202/300 [00:34<00:16,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I']\n",
      "Instead, you must utilize Instagram proxies and proportional strategies so as to earn money with the social network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 211/300 [00:35<00:14,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I']\n",
      "And they add: \"OMNINET's product portfolio is both innovative and mature.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 222/300 [00:37<00:12,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "Have an enthusiastic attitude\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 232/300 [00:38<00:09,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['K K']\n",
      "Royal Dutch Shell: claims for oil pollution in Nigeria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 242/300 [00:40<00:09,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I I']\n",
      "Scalping is highly delicate trading, that almost can’t be formally characterized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 251/300 [00:41<00:08,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I I I']\n",
      "Battling on this video slot, you will get an incredible dose of excitement, as the victory always brings great pleasure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 262/300 [00:43<00:05,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I']\n",
      "Not all diseases are easily seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 272/300 [00:45<00:03,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I I']\n",
      "Nel-Peters is from the South African coastal community of Sedgefield in the Western Cape province.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 281/300 [00:46<00:02,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I I I I I']\n",
      "The 2008 financial crisis put them out of business.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 292/300 [00:51<00:05,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................']\n",
      "… the more I like it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:52<00:00,  5.71it/s]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_1.2B\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_1.2B\").to(device)\n",
    "\n",
    "# parameters_to_prune = [(m, \"weight\") for m in filter(lambda m: hasattr(m, \"weight\"), model.modules())]\n",
    "# parameters_to_prune = [(m[1], \"weight\") for m in filter(lambda nm: hasattr(nm[1], \"weight\") and \"mlp.c_fc\" in nm[0], model.named_modules())]\n",
    "\n",
    "parameters_to_prune = [(m[1], \"weight\") for m in model.named_modules() if \"fc\" in m[0] and m[1].out_features==8192]\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.9,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "# test = load_dataset(\"yhavinga/ccmatrix\", \"de-en\", split=\"train\", streaming=True).take(1000)\n",
    "# \n",
    "test = load_dataset(\"j0hngou/ccmatrix_de-en\", split=\"train\")[:300]\n",
    "tokenizer.src_lang = \"de\"\n",
    "\n",
    "de_samples = [x[\"de\"] for x in test[\"translation\"]]\n",
    "en_samples = [x[\"en\"] for x in test[\"translation\"]]\n",
    "\n",
    "# encodings = tokenizer(\"\\n\\n\".join([x[\"de\"] for x in test[\"translation\"]]), return_tensors=\"pt\")\n",
    "# out_encodings = tokenizer(\"\\n\\n\".join([x[\"en\"] for x in test[\"translation\"]]), return_tensors=\"pt\")\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "bleu_out = []\n",
    "\n",
    "all_translations = []\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "for idx in tqdm(range(len(de_samples))):\n",
    "    tokenized = tokenizer(de_samples[idx], return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**tokenized, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "    predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    all_translations.append(predictions[0])\n",
    "    \n",
    "    if (idx % 10 == 0):\n",
    "        print (predictions)\n",
    "        print(en_samples[idx])\n",
    "    \n",
    "results = bleu.compute(predictions=all_translations, references=en_samples)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('„Ich werde dir nun zum Ausgleich für alles, was du mir gesagt hast, ein paar Informationen geben.',\n",
       " '“I’m going to give you some information to compensate for everything you told me.',\n",
       " '“I am giving you some information now, in return for all that you have given me.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_idx = 3\n",
    "de_samples[t_idx], all_translations[t_idx], en_samples[t_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [0.011693548387096775, 0.0, 0.0, 0.0],\n",
       " 'brevity_penalty': 0.31920061590833076,\n",
       " 'length_ratio': 0.46686746987951805,\n",
       " 'translation_length': 2480,\n",
       " 'reference_length': 5312}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 1.2B \n",
    "0%\n",
    "{'bleu': 0.40251797333058786,\n",
    " 'precisions': [0.6805947675512893,\n",
    "  0.4606024336724516,\n",
    "  0.3350307659664757,\n",
    "  0.24994334919555858],\n",
    " 'brevity_penalty': 1.0,\n",
    " 'length_ratio': 1.0001882530120483,\n",
    " 'translation_length': 5313,\n",
    " 'reference_length': 5312}\n",
    "\n",
    "10%\n",
    "{'bleu': 0.39995484202295456,\n",
    " 'precisions': [0.6793375987956342,\n",
    "  0.4583167132030315,\n",
    "  0.3322019516334323,\n",
    "  0.24739465337562302],\n",
    " 'brevity_penalty': 1.0,\n",
    " 'length_ratio': 1.0003765060240963,\n",
    " 'translation_length': 5314,\n",
    " 'reference_length': 5312}\n",
    "\n",
    "##50\n",
    "{'bleu': 0.37629659926239045,\n",
    " 'precisions': [0.6697683251044436,\n",
    "  0.4424083769633508,\n",
    "  0.3116159451350193,\n",
    "  0.22486833066178155],\n",
    " 'brevity_penalty': 0.9913027587878498,\n",
    " 'length_ratio': 0.9913403614457831,\n",
    " 'translation_length': 5266,\n",
    " 'reference_length': 5312}\n",
    " \n",
    " ##90\n",
    "{'bleu': 0.0,\n",
    " 'precisions': [0.011693548387096775, 0.0, 0.0, 0.0],\n",
    " 'brevity_penalty': 0.31920061590833076,\n",
    " 'length_ratio': 0.46686746987951805,\n",
    " 'translation_length': 2480,\n",
    " 'reference_length': 5312}\n",
    "\n",
    "## FB_M2M = []\n",
    "## 0% \n",
    "{'bleu': 0.3560997086090464,\n",
    " 'precisions': [0.6521335807050093,\n",
    "  0.4165029469548134,\n",
    "  0.2870563674321503,\n",
    "  0.20623608017817371],\n",
    " 'brevity_penalty': 1.0,\n",
    " 'length_ratio': 1.014683734939759,\n",
    " 'translation_length': 5390,\n",
    " 'reference_length': 5312}\n",
    " \n",
    "## 10% \n",
    "\n",
    "{'bleu': 0.35692963275594786,\n",
    " 'precisions': [0.6527881040892193,\n",
    "  0.41771653543307086,\n",
    "  0.28765690376569036,\n",
    "  0.20691964285714284],\n",
    " 'brevity_penalty': 1.0,\n",
    " 'length_ratio': 1.0128012048192772,\n",
    " 'translation_length': 5380,\n",
    " 'reference_length': 5312}\n",
    " \n",
    "## 50%\n",
    "{'bleu': 0.0,\n",
    " 'precisions': [0.0, 0.0, 0.0, 0.0],\n",
    " 'brevity_penalty': 1.0,\n",
    " 'length_ratio': 2.907191265060241,\n",
    " 'translation_length': 15443,\n",
    " 'reference_length': 5312}\n",
    "\n",
    "## 90%\n",
    "{'bleu': 0.0,\n",
    " 'precisions': [0.0036163714111178986, 0.0, 0.0, 0.0],\n",
    " 'brevity_penalty': 1.0,\n",
    " 'length_ratio': 7.704254518072289,\n",
    " 'translation_length': 40925,\n",
    " 'reference_length': 5312}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/home/christopherkang/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████| 2/2 [00:00<00:00, 996.75it/s]\n",
      "100%|██████████| 300/300 [01:19<00:00,  3.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "parameters_to_prune = [(m, \"weight\") for m in filter(lambda m: hasattr(m, \"weight\"), model.modules())]\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.99,\n",
    ")\n",
    "\n",
    "NUM_TRIALS = 300\n",
    "\n",
    "# dataset = load_dataset(\"squad_v2\")\n",
    "dataset = load_dataset(\"squad\")\n",
    "all_outputs = []\n",
    "all_answers = []\n",
    "for idx in tqdm(range(NUM_TRIALS)):\n",
    "    question, text = dataset[\"validation\"][idx][\"question\"], dataset[\"validation\"][idx][\"context\"]\n",
    "    \n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    \n",
    "    \n",
    "    {'': '1976', 'id': '56e10a3be3433e1400422b22', 'no_answer_probability': 0.}\n",
    "    all_outputs.append({\n",
    "        \"prediction_text\": tokenizer.decode(predict_answer_tokens, skip_special_tokens=True),\n",
    "        \"id\": dataset[\"validation\"][idx][\"id\"],\n",
    "        \"no_answer_probability\": 0.,\n",
    "    })\n",
    "    all_answers.append({\n",
    "        \"answers\": dataset[\"validation\"][idx][\"answers\"],\n",
    "        \"id\": dataset[\"validation\"][idx][\"id\"],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prediction_text': '',\n",
       "  'id': '56be4db0acb8001400a502ec',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be4db0acb8001400a502ed',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be4db0acb8001400a502ee',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be4db0acb8001400a502ef',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be4db0acb8001400a502f0',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be8e613aeaaa14008c90d1',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be8e613aeaaa14008c90d2',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be8e613aeaaa14008c90d3',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bea9923aeaaa14008c91b9',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bea9923aeaaa14008c91ba',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bea9923aeaaa14008c91bb',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beace93aeaaa14008c91df',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beace93aeaaa14008c91e0',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beace93aeaaa14008c91e1',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beace93aeaaa14008c91e2',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beace93aeaaa14008c91e3',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf10f43aeaaa14008c94fd',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf10f43aeaaa14008c94fe',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf10f43aeaaa14008c94ff',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf10f43aeaaa14008c9500',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf10f43aeaaa14008c9501',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d20362e7d4791d009025e8',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d20362e7d4791d009025e9',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d20362e7d4791d009025ea',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d20362e7d4791d009025eb',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d600e31c85041400946eae',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d600e31c85041400946eb0',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d600e31c85041400946eb1',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d9895ddc89441400fdb50e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d9895ddc89441400fdb510',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be4e1facb8001400a502f6',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be4e1facb8001400a502f9',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be4e1facb8001400a502fa',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beaa4a3aeaaa14008c91c2',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beaa4a3aeaaa14008c91c3',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bead5a3aeaaa14008c91e9',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bead5a3aeaaa14008c91ea',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bead5a3aeaaa14008c91eb',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bead5a3aeaaa14008c91ec',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bead5a3aeaaa14008c91ed',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf159b3aeaaa14008c9507',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf159b3aeaaa14008c9508',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf159b3aeaaa14008c9509',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf159b3aeaaa14008c950a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf159b3aeaaa14008c950b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d2045de7d4791d009025f3',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d2045de7d4791d009025f4',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d2045de7d4791d009025f5',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d2045de7d4791d009025f6',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6017d1c85041400946ebe',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6017d1c85041400946ec1',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6017d1c85041400946ec2',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98a59dc89441400fdb52a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98a59dc89441400fdb52b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98a59dc89441400fdb52e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be4eafacb8001400a50302',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be4eafacb8001400a50303',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be4eafacb8001400a50304',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beab833aeaaa14008c91d2',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beab833aeaaa14008c91d3',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beab833aeaaa14008c91d4',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beae423aeaaa14008c91f4',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beae423aeaaa14008c91f5',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beae423aeaaa14008c91f6',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beae423aeaaa14008c91f7',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf17653aeaaa14008c9511',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf17653aeaaa14008c9513',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf17653aeaaa14008c9514',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf17653aeaaa14008c9515',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d204ade7d4791d00902603',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d204ade7d4791d00902604',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d601e41c85041400946ece',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d601e41c85041400946ecf',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d601e41c85041400946ed0',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d601e41c85041400946ed1',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d601e41c85041400946ed2',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98b33dc89441400fdb53b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98b33dc89441400fdb53c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98b33dc89441400fdb53d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98b33dc89441400fdb53e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be5333acb8001400a5030a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be5333acb8001400a5030b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be5333acb8001400a5030c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be5333acb8001400a5030d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be5333acb8001400a5030e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beaf5e3aeaaa14008c91fd',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beaf5e3aeaaa14008c91fe',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beaf5e3aeaaa14008c91ff',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beaf5e3aeaaa14008c9200',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beaf5e3aeaaa14008c9201',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf1ae93aeaaa14008c951b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf1ae93aeaaa14008c951c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf1ae93aeaaa14008c951e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf1ae93aeaaa14008c951f',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d2051ce7d4791d00902608',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d2051ce7d4791d00902609',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d2051ce7d4791d0090260a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d2051ce7d4791d0090260b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d602631c85041400946ed8',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d602631c85041400946eda',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d602631c85041400946edb',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d602631c85041400946edc',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98c53dc89441400fdb544',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98c53dc89441400fdb545',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98c53dc89441400fdb546',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98c53dc89441400fdb548',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be53b8acb8001400a50314',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be53b8acb8001400a50315',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be53b8acb8001400a50316',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beafca3aeaaa14008c9207',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beafca3aeaaa14008c9208',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf42f53aeaaa14008c95a3',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d2053ae7d4791d00902610',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6edd00d65d21400198250',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6edd00d65d21400198251',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98d0adc89441400fdb54e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98d0adc89441400fdb54f',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be5438acb8001400a5031a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be5438acb8001400a5031b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be5438acb8001400a5031c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb03c3aeaaa14008c920b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb03c3aeaaa14008c920d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf3c633aeaaa14008c9580',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf3c633aeaaa14008c9581',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf3c633aeaaa14008c9582',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d20564e7d4791d00902612',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6ee6e0d65d21400198254',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6ee6e0d65d21400198255',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6ee6e0d65d21400198256',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6ee6e0d65d21400198257',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6ee6e0d65d21400198258',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98db6dc89441400fdb552',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98db6dc89441400fdb553',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98db6dc89441400fdb554',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be54bdacb8001400a50322',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be54bdacb8001400a50323',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be54bdacb8001400a50324',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be54bdacb8001400a50325',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be54bdacb8001400a50326',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb0f43aeaaa14008c921b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb0f43aeaaa14008c921c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb0f43aeaaa14008c921d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb0f43aeaaa14008c921e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb0f43aeaaa14008c921f',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf21b43aeaaa14008c9525',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf21b43aeaaa14008c9526',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf21b43aeaaa14008c9528',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf21b43aeaaa14008c9529',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6ef6a0d65d21400198260',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6ef6a0d65d21400198262',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98f0ddc89441400fdb558',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98f0ddc89441400fdb559',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98f0ddc89441400fdb55a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98f0ddc89441400fdb55b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98f0ddc89441400fdb55c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be5523acb8001400a5032c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be5523acb8001400a5032d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be5523acb8001400a5032e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be5523acb8001400a5032f',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be5523acb8001400a50330',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb2153aeaaa14008c9225',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb2153aeaaa14008c9226',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb2153aeaaa14008c9227',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb2153aeaaa14008c9228',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb2153aeaaa14008c9229',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf23363aeaaa14008c952f',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf23363aeaaa14008c9530',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf23363aeaaa14008c9531',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf23363aeaaa14008c9532',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf23363aeaaa14008c9533',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f0770d65d21400198268',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f0770d65d21400198269',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f0770d65d2140019826a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f0770d65d2140019826c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98fbfdc89441400fdb562',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98fbfdc89441400fdb563',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98fbfdc89441400fdb564',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d98fbfdc89441400fdb565',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be572b3aeaaa14008c9052',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb2a03aeaaa14008c922f',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb2a03aeaaa14008c9230',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb2a03aeaaa14008c9231',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb2a03aeaaa14008c9232',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb2a03aeaaa14008c9233',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf28c73aeaaa14008c9539',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf28c73aeaaa14008c953a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf28c73aeaaa14008c953c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf28c73aeaaa14008c953d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f1190d65d21400198272',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f1190d65d21400198273',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f1190d65d21400198274',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f1190d65d21400198275',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f1190d65d21400198276',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d99179dc89441400fdb56c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d99179dc89441400fdb56d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d99179dc89441400fdb570',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be59683aeaaa14008c9058',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be59683aeaaa14008c9059',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56be59683aeaaa14008c905a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb3083aeaaa14008c923d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb3083aeaaa14008c923e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb3083aeaaa14008c923f',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb3083aeaaa14008c9240',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb3083aeaaa14008c9241',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf2afe3aeaaa14008c9543',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf2afe3aeaaa14008c9544',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf2afe3aeaaa14008c9545',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf2afe3aeaaa14008c9547',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f2000d65d2140019827c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f2000d65d2140019827d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f2000d65d2140019827e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f2000d65d2140019827f',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d9943fdc89441400fdb576',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d9943fdc89441400fdb577',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d9943fdc89441400fdb578',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d9943fdc89441400fdb57a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb3a03aeaaa14008c9247',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb3a03aeaaa14008c9248',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb3a03aeaaa14008c9249',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb3a03aeaaa14008c924a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb3a03aeaaa14008c924b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf6b303aeaaa14008c960b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf6b303aeaaa14008c960c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf6b303aeaaa14008c960d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf6b303aeaaa14008c960e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf6b303aeaaa14008c960f',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f2960d65d21400198286',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f2960d65d21400198287',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f2960d65d21400198288',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f2960d65d21400198289',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f2960d65d2140019828a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d997cddc89441400fdb586',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d997cddc89441400fdb587',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d997cddc89441400fdb588',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d997cddc89441400fdb589',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d997cddc89441400fdb58a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb4343aeaaa14008c925b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb4343aeaaa14008c925c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb4343aeaaa14008c925d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb4343aeaaa14008c925e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb4343aeaaa14008c925f',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f3500d65d21400198290',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f3500d65d21400198291',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f3500d65d21400198292',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f3500d65d21400198293',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f3500d65d21400198294',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d9992fdc89441400fdb59c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d9992fdc89441400fdb59e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d9992fdc89441400fdb59f',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d9992fdc89441400fdb5a0',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb4e43aeaaa14008c9265',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb4e43aeaaa14008c9266',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb4e43aeaaa14008c9267',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb4e43aeaaa14008c9268',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb4e43aeaaa14008c9269',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf301c3aeaaa14008c954d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf301c3aeaaa14008c954e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf301c3aeaaa14008c954f',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf301c3aeaaa14008c9550',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf301c3aeaaa14008c9551',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f4030d65d2140019829a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f4030d65d2140019829b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f4030d65d2140019829c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f4030d65d2140019829d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6f4030d65d2140019829e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d99b7bdc89441400fdb5c8',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d99b7bdc89441400fdb5c9',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d99b7bdc89441400fdb5ca',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d99b7bdc89441400fdb5cb',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d99b7bdc89441400fdb5cc',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb57b3aeaaa14008c9279',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb57b3aeaaa14008c927a',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb57b3aeaaa14008c927b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb57b3aeaaa14008c927c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb57b3aeaaa14008c927d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf38383aeaaa14008c956b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf38383aeaaa14008c956c',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf38383aeaaa14008c956d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf38383aeaaa14008c956e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf38383aeaaa14008c956f',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6fe0b0d65d214001982a4',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6fe0b0d65d214001982a5',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6fe0b0d65d214001982a6',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6fe0b0d65d214001982a7',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d99c44dc89441400fdb5d6',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d99c44dc89441400fdb5d7',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d99c44dc89441400fdb5d8',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d99c44dc89441400fdb5d9',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d99c44dc89441400fdb5da',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb6533aeaaa14008c928d',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb6533aeaaa14008c928e',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb6533aeaaa14008c9290',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56beb6533aeaaa14008c9291',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf3e803aeaaa14008c9588',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf3e803aeaaa14008c9589',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56bf3e803aeaaa14008c958b',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6fea90d65d214001982ae',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6fea90d65d214001982af',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6fea90d65d214001982b0',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'prediction_text': '',\n",
       "  'id': '56d6fea90d65d214001982b2',\n",
       "  'no_answer_probability': 0.0}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "squad_v2_ = load(\"squad_v2\")\n",
    "results = squad_v2_.compute(predictions=all_outputs, references=all_answers)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###\n",
    "0% \n",
    "{'exact': 86.33333333333333,\n",
    " 'f1': 89.75174603174605,\n",
    " 'total': 300,\n",
    " 'HasAns_exact': 86.33333333333333,\n",
    " 'HasAns_f1': 89.75174603174605,\n",
    " 'HasAns_total': 300,\n",
    " 'best_exact': 86.33333333333333,\n",
    " 'best_exact_thresh': 0.0,\n",
    " 'best_f1': 89.75174603174605,\n",
    " 'best_f1_thresh': 0.0}\n",
    " \n",
    " 10%\n",
    " {'exact': 86.33333333333333,\n",
    " 'f1': 89.93525480367586,\n",
    " 'total': 300,\n",
    " 'HasAns_exact': 86.33333333333333,\n",
    " 'HasAns_f1': 89.93525480367586,\n",
    " 'HasAns_total': 300,\n",
    " 'best_exact': 86.33333333333333,\n",
    " 'best_exact_thresh': 0.0,\n",
    " 'best_f1': 89.93525480367586,\n",
    " 'best_f1_thresh': 0.0}\n",
    " \n",
    " 50%\n",
    " {'exact': 0.3333333333333333,\n",
    " 'f1': 5.101793703923397,\n",
    " 'total': 300,\n",
    " 'HasAns_exact': 0.3333333333333333,\n",
    " 'HasAns_f1': 5.101793703923397,\n",
    " 'HasAns_total': 300,\n",
    " 'best_exact': 0.3333333333333333,\n",
    " 'best_exact_thresh': 0.0,\n",
    " 'best_f1': 5.101793703923397,\n",
    " 'best_f1_thresh': 0.0}\n",
    " \n",
    " \n",
    " 90%\n",
    " {'exact': 0.3333333333333333,\n",
    " 'f1': 2.5867006975238365,\n",
    " 'total': 300,\n",
    " 'HasAns_exact': 0.3333333333333333,\n",
    " 'HasAns_f1': 2.5867006975238365,\n",
    " 'HasAns_total': 300,\n",
    " 'best_exact': 0.3333333333333333,\n",
    " 'best_exact_thresh': 0.0,\n",
    " 'best_f1': 2.5867006975238365,\n",
    " 'best_f1_thresh': 0.0}\n",
    " \n",
    " \n",
    " 95%\n",
    " {'exact': 0.0,\n",
    " 'f1': 4.257774368459605,\n",
    " 'total': 300,\n",
    " 'HasAns_exact': 0.0,\n",
    " 'HasAns_f1': 4.257774368459605,\n",
    " 'HasAns_total': 300,\n",
    " 'best_exact': 0.0,\n",
    " 'best_exact_thresh': 0.0,\n",
    " 'best_f1': 4.257774368459605,\n",
    " 'best_f1_thresh': 0.0}\n",
    " \n",
    " 99%\n",
    " {'exact': 0.0,\n",
    " 'f1': 0.0,\n",
    " 'total': 300,\n",
    " 'HasAns_exact': 0.0,\n",
    " 'HasAns_f1': 0.0,\n",
    " 'HasAns_total': 300,\n",
    " 'best_exact': 0.0,\n",
    " 'best_exact_thresh': 0.0,\n",
    " 'best_f1': 0.0,\n",
    " 'best_f1_thresh': 0.0}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 0.0,\n",
       " 'f1': 0.0,\n",
       " 'total': 300,\n",
       " 'HasAns_exact': 0.0,\n",
       " 'HasAns_f1': 0.0,\n",
       " 'HasAns_total': 300,\n",
       " 'best_exact': 0.0,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 0.0,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 66.66666666666667,\n",
       " 'f1': 66.66666666666667,\n",
       " 'total': 3,\n",
       " 'HasAns_exact': 66.66666666666667,\n",
       " 'HasAns_f1': 66.66666666666667,\n",
       " 'HasAns_total': 3,\n",
       " 'best_exact': 66.66666666666667,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 66.66666666666667,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "squad_metric = load(\"squad_v2\")\n",
    "predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22', 'no_answer_probability': 0.}, {'prediction_text': 'Beyonce', 'id': '56d2051ce7d4791d0090260b', 'no_answer_probability': 0.},  {'prediction_text': 'climate change', 'id': '5733b5344776f419006610e1', 'no_answer_probability': 0.}]\n",
    "references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}, {'answers': {'answer_start': [233], 'text': ['Beyoncé and Bruno Mars']}, 'id': '56d2051ce7d4791d0090260b'}, {'answers': {'answer_start': [891], 'text': ['climate change']}, 'id': '5733b5344776f419006610e1'}]\n",
    "results = squad_metric.compute(predictions=predictions, references=references)\n",
    "results\n",
    "{'exact': 66.66666666666667, 'f1': 66.66666666666667, 'total': 3, 'HasAns_exact': 66.66666666666667, 'HasAns_f1': 66.66666666666667, 'HasAns_total': 3, 'best_exact': 66.66666666666667, 'best_exact_thresh': 0.0, 'best_f1': 66.66666666666667, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter \n",
    "\n",
    "[TwHIN-BERT](https://huggingface.co/Twitter/twhin-bert-large?text=The+goal+of+life+is+%3Cmask%3E.)\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/2209.07562.pdf)\n",
    "\n",
    "benchmark -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:  68%|██████▊   | 1.53G/2.25G [03:34<01:40, 7.12MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('Twitter/twhin-bert-large')\n",
    "model = BertForMaskedLM.from_pretrained('Twitter/twhin-bert-large')\n",
    "inputs = tokenizer(\"I'm using TwHIN-BERT! #TwHIN-BERT #NLP\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'dict' object cannot be converted to 'Sequence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ff7beec22cf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/llm/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3438\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3439\u001b[0m             \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3440\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3441\u001b[0m         )\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/llm/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'dict' object cannot be converted to 'Sequence'"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'city'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Twitter/twhin-bert-large\")\n",
    "\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"Twitter/twhin-bert-large\")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-large-cased\")\n",
    "\n",
    "inputs = tokenizer(\"The [MASK] is falling!\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeBERTa\n",
    "\n",
    "[Q&A](https://huggingface.co/docs/transformers/tasks/question_answering)\n",
    "\n",
    "[Q&A eval](https://huggingface.co/course/chapter7/7?fw=tf)\n",
    "\n",
    "-- had problems just evaluating the model, wasn't satisfied with performance, so switching to a lower parameter model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 633/633 [00:00<00:00, 1.45MB/s]\n",
      "Some weights of the model checkpoint at microsoft/deberta-v2-xxlarge were not used when initializing DebertaV2ForQuestionAnswering: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v2-xxlarge and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Found cached dataset squad (/home/christopherkang/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████| 2/2 [00:00<00:00, 1054.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaV2Tokenizer, DebertaV2ForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v2-xxlarge\")\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"hf-internal-testing/tiny-random-deberta-v2\")\n",
    "model = DebertaV2ForQuestionAnswering.from_pretrained(\"microsoft/deberta-v2-xxlarge\", force_download=True)\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "\n",
    "t_idx = 1\n",
    "# question, text = dataset[\"train\"][t_idx][\"question\"], dataset[\"train\"][t_idx][\"context\"]\n",
    "# print(dataset[\"train\"][t_idx][\"answers\"])\n",
    "question, text = \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\", \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\"\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "print(tokenizer.decode(predict_answer_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading: 100%|██████████| 633/633 [00:00<00:00, 1.51MB/s]\n",
      "Some weights of the model checkpoint at microsoft/deberta-v2-xxlarge were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v2-xxlarge and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LTO'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DebertaV2Tokenizer, DebertaV2ForMaskedLM\n",
    "import torch\n",
    "\n",
    "# tokenizer = DebertaV2Tokenizer.from_pretrained(\"hf-internal-testing/tiny-random-deberta-v2\")\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v2-xxlarge\")\n",
    "# model = DebertaV2ForMaskedLM.from_pretrained(\"hf-internal-testing/tiny-random-deberta-v2\")\n",
    "model = DebertaV2ForMaskedLM.from_pretrained(\"microsoft/deberta-v2-xxlarge\", force_download=True)\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors = \"pt\")\n",
    "# inputs = tokenizer(\"I like to put [MASK] and mustard on my hot dog.\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(127)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v2-xxlarge were not used when initializing DebertaV2ForQuestionAnswering: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v2-xxlarge and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Found cached dataset squad (/home/christopherkang/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████| 2/2 [00:00<00:00, 957.71it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      " 91%|█████████ | 10/11 [00:04<00:00,  2.20ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([[    1, 32251,  1485, ...,     4,   287,     2],\n",
      "       [    1, 32251,  1485, ...,  1582,  2616,     2],\n",
      "       [    1, 32251,  1485, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [    1,  2264,    80, ...,     0,     0,     0],\n",
      "       [    1, 12375,  2308, ...,     0,     0,     0],\n",
      "       [    1, 12375, 27724, ...,     4,     2,     0]]), 'attention_mask': array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_expand_dims_dispatcher() missing 1 required positional argument: 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b6cbfd2f16ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# outputs = model(**batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mexpand_dims\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _expand_dims_dispatcher() missing 1 required positional argument: 'axis'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, DebertaTokenizerFast\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"microsoft/deberta-v2-xxlarge\", )\n",
    "\n",
    "def preprocess_validation_examples(examples, max_length=100, stride=50):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs\n",
    "\n",
    "## taken from HF\n",
    "\n",
    "raw_datasets = load_dataset(\"squad\")\n",
    "small_eval_set = raw_datasets[\"validation\"].select(range(100))\n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "tokenizer = DebertaTokenizerFast.from_pretrained(\"microsoft/deberta-base\")\n",
    "eval_set = small_eval_set.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "\n",
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)\n",
    "\n",
    "\n",
    "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\", \"token_type_ids\"])\n",
    "eval_set_for_model.set_format(\"numpy\")\n",
    "\n",
    "batch = {k: eval_set_for_model[k] for k in eval_set_for_model.column_names}\n",
    "print(batch)\n",
    "\n",
    "# outputs = model(**batch)\n",
    "outputs = model(np.expand_dims(batch[\"input_ids\"][0]), batch[\"attention_mask\"][0])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "for example in small_eval_set:\n",
    "    example_id = example[\"id\"]\n",
    "    context = example[\"context\"]\n",
    "    answers = []\n",
    "\n",
    "    for feature_index in example_to_features[example_id]:\n",
    "        start_logit = start_logits[feature_index]\n",
    "        end_logit = end_logits[feature_index]\n",
    "        offsets = eval_set[\"offset_mapping\"][feature_index]\n",
    "\n",
    "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # Skip answers that are not fully in the context\n",
    "                if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                    continue\n",
    "                # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                if (\n",
    "                    end_index < start_index\n",
    "                    or end_index - start_index + 1 > max_answer_length\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                answers.append(\n",
    "                    {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"attention_mask\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-8ed7ae7a61c1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-8ed7ae7a61c1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [**batch]\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[**batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "performance = metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = params[\"transformer.h.46.attn.bias\"].reshape((-1)).size()[0]\n",
    "sum(np.isclose(params[\"transformer.h.46.attn.bias\"].reshape((-1)), np.zeros(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_sparsity(model):\n",
    "    params = model.state_dict()\n",
    "    out = pd.DataFrame(index = params.keys(), columns=[\"type\", \"weights\", \"zero\", \"%\"])\n",
    "\n",
    "    for key, value in params.items():\n",
    "    # print(key)\n",
    "        flattened = value.reshape((-1))\n",
    "        size = flattened.size()[0]\n",
    "\n",
    "        type_of_arr = str(key).split(\".\")[-1]\n",
    "\n",
    "        out.loc[key, \"type\"] = type_of_arr\n",
    "        out.loc[key, \"weights\"] = size\n",
    "        out.loc[key, \"zero\"] = np.count_nonzero(np.isclose(flattened, np.zeros(size), atol=1e-2))\n",
    "\n",
    "    out[\"%\"] = out[\"zero\"] / out[\"weights\"]\n",
    "    return out\n",
    "\n",
    "  ## for weights in model\n",
    "  ## sparsity[\"layer_name\"] = sparsity_fn(weights)\n",
    "  ## return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_sparsity = assess_sparsity(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "We are looking for three HuggingFace models all with >1B parameters. For each of the categories:\n",
    "* E-o: [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://huggingface.co/microsoft/deberta-v2-xxlarge)\n",
    "* D-o: [GPT2-XL](https://huggingface.co/gpt2-xl)\n",
    "* E-D: [Facebook / M2M100](https://huggingface.co/facebook/m2m100_418M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_sparsity_2 = assess_sparsity(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_sparsity_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune = [(m, \"weight\") for m in filter(lambda m: hasattr(m, \"weight\"), model.modules())]\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(\"gpt2-xl-10\")\n",
    "model.push_to_hub(\"gpt2-xl-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "generator(\n",
    "    \"My name is Julien and I like to\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_MODELS = [\"gpt2-xl\", \"microsoft/deberta-v2-xxlarge\", \"facebook/m2m100_418M\"]\n",
    "def produce_model(name):\n",
    "    if name == \"gpt2-xl\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\", force_download=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\", force_download=True)\n",
    "        \n",
    "        return tokenizer, model\n",
    "        \n",
    "    elif name == \"microsoft/deberta-v2-xxlarge\":\n",
    "        model = AutoModel.from_pretrained(\"microsoft/deberta-v2-xxlarge\", force_download=True)\n",
    "        return _, model\n",
    "    \n",
    "    elif name == \"facebook/m2m100_418M\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\", force_download=True)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\", force_download=True)\n",
    "\n",
    "        return tokenizer, model\n",
    "    \n",
    "    assert 1 == 0, \"Unknown name!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.5\n",
      "0.9\n",
      "0.95\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "for sparsity in np.array([10, 50, 90, 95, 99]) / 100:\n",
    "    print(sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"My name is Julien and I like to write on stories that are interesting for me, like I have several stories that I'm reading right now where there are a lot of girls who are doing the same thing as me, but nobody was interested in\"}]\n",
      "0.1 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'My name is Julien and I like to English Author Author User Apprentice Senior Teacher Independent\\n Woman One Other Year Writer Year Woman Year Woman Woman World Word Author Other Teacher Teacher Teacher Self Teacher Teacher Teacher Teacher Teacher Teacher Teacher Teacher U Teacher Writer\\n Year Teacher'}]\n",
      "0.5 done!\n"
     ]
    }
   ],
   "source": [
    "SPARSITY = np.array([10, 50, 90, 95, 99]) / 100\n",
    "# SPARSITY = np.array([99]) / 100\n",
    "MODEL = LIST_OF_MODELS[0]\n",
    "\n",
    "for sparsity in SPARSITY:\n",
    "    tokenizer, model = produce_model(MODEL)\n",
    "    \n",
    "    parameters_to_prune = [(m, \"weight\") for m in filter(lambda m: hasattr(m, \"weight\"), model.modules())]\n",
    "\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=sparsity,\n",
    "    )\n",
    "    \n",
    "    repo_header = MODEL.split(\"/\")[-1]\n",
    "    \n",
    "    generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    print(generator(\"My name is Julien and I like to\", ))\n",
    "    \n",
    "    if tokenizer:\n",
    "        tokenizer.push_to_hub(f\"b_{repo_header}_{int(sparsity * 100)}\")\n",
    "\n",
    "    model.push_to_hub(f\"b_{repo_header}_{int(sparsity * 100)}\")\n",
    "    \n",
    "    del tokenizer\n",
    "    del model\n",
    "    del parameters_to_prune\n",
    "    \n",
    "    print(f\"{sparsity} done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## grab model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "## prune\n",
    "parameters_to_prune = [(m, \"weight\") for m in filter(lambda m: hasattr(m, \"weight\"), model.modules())]\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.4,\n",
    ")\n",
    "\n",
    "## test\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "print(generator(\"My name is Julien and I like to\", ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ctkang/test_b/commit/2472028eb83ca95a7478d7282addfd3bfd53b5e6', commit_message='Upload model', commit_description='', oid='2472028eb83ca95a7478d7282addfd3bfd53b5e6', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"test_b\")\n",
    "model.push_to_hub(\"test_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The following sections can be used to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GPT2LMHeadModel(\n",
       "   (transformer): GPT2Model(\n",
       "     (wte): Embedding(50257, 1600)\n",
       "     (wpe): Embedding(1024, 1600)\n",
       "     (drop): Dropout(p=0.1, inplace=False)\n",
       "     (h): ModuleList(\n",
       "       (0): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (1): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (2): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (4): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (5): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (6): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (7): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (8): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (9): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (10): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (11): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (12): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (13): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (14): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (15): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (16): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (17): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (18): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (19): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (20): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (21): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (22): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (23): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (24): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (25): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (26): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (27): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (28): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (29): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (30): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (31): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (32): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (33): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (34): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (35): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (36): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (37): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (38): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (39): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (40): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (41): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (42): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (43): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (44): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (45): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (46): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (47): GPT2Block(\n",
       "         (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       " ),\n",
       " GPT2Model(\n",
       "   (wte): Embedding(50257, 1600)\n",
       "   (wpe): Embedding(1024, 1600)\n",
       "   (drop): Dropout(p=0.1, inplace=False)\n",
       "   (h): ModuleList(\n",
       "     (0): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (1): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (2): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (3): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (4): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (5): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (6): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (7): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (8): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (9): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (10): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (11): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (12): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (13): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (14): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (15): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (16): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (17): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (18): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (19): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (20): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (21): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (22): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (23): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (24): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (25): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (26): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (27): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (28): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (29): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (30): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (31): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (32): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (33): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (34): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (35): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (36): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (37): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (38): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (39): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (40): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (41): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (42): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (43): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (44): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (45): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (46): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (47): GPT2Block(\n",
       "       (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (attn): GPT2Attention(\n",
       "         (c_attn): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): GPT2MLP(\n",
       "         (c_fc): Conv1D()\n",
       "         (c_proj): Conv1D()\n",
       "         (act): NewGELUActivation()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       " ),\n",
       " Embedding(50257, 1600),\n",
       " Embedding(1024, 1600),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " ModuleList(\n",
       "   (0): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (1): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (2): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (3): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (4): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (5): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (6): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (7): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (8): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (9): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (10): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (11): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (12): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (13): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (14): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (15): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (16): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (17): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (18): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (19): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (20): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (21): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (22): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (23): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (24): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (25): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (26): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (27): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (28): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (29): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (30): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (31): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (32): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (33): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (34): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (35): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (36): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (37): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (38): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (39): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (40): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (41): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (42): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (43): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (44): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (45): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (46): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (47): GPT2Block(\n",
       "     (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (attn): GPT2Attention(\n",
       "       (c_attn): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "       (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "     (mlp): GPT2MLP(\n",
       "       (c_fc): Conv1D()\n",
       "       (c_proj): Conv1D()\n",
       "       (act): NewGELUActivation()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " GPT2Block(\n",
       "   (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (attn): GPT2Attention(\n",
       "     (c_attn): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "     (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "   (mlp): GPT2MLP(\n",
       "     (c_fc): Conv1D()\n",
       "     (c_proj): Conv1D()\n",
       "     (act): NewGELUActivation()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2Attention(\n",
       "   (c_attn): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "   (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " GPT2MLP(\n",
       "   (c_fc): Conv1D()\n",
       "   (c_proj): Conv1D()\n",
       "   (act): NewGELUActivation()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " Conv1D(),\n",
       " Conv1D(),\n",
       " NewGELUActivation(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " LayerNorm((1600,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=1600, out_features=50257, bias=False)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\")\n",
    "# [(m, \"weight\") for m in filter(lambda m: hasattr(m, \"weight\"), model.modules())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'transformer',\n",
       " 'transformer.wte',\n",
       " 'transformer.wpe',\n",
       " 'transformer.drop',\n",
       " 'transformer.h',\n",
       " 'transformer.h.0',\n",
       " 'transformer.h.0.ln_1',\n",
       " 'transformer.h.0.attn',\n",
       " 'transformer.h.0.attn.c_attn',\n",
       " 'transformer.h.0.attn.c_proj',\n",
       " 'transformer.h.0.attn.attn_dropout',\n",
       " 'transformer.h.0.attn.resid_dropout',\n",
       " 'transformer.h.0.ln_2',\n",
       " 'transformer.h.0.mlp',\n",
       " 'transformer.h.0.mlp.c_fc',\n",
       " 'transformer.h.0.mlp.c_proj',\n",
       " 'transformer.h.0.mlp.act',\n",
       " 'transformer.h.0.mlp.dropout',\n",
       " 'transformer.h.1',\n",
       " 'transformer.h.1.ln_1',\n",
       " 'transformer.h.1.attn',\n",
       " 'transformer.h.1.attn.c_attn',\n",
       " 'transformer.h.1.attn.c_proj',\n",
       " 'transformer.h.1.attn.attn_dropout',\n",
       " 'transformer.h.1.attn.resid_dropout',\n",
       " 'transformer.h.1.ln_2',\n",
       " 'transformer.h.1.mlp',\n",
       " 'transformer.h.1.mlp.c_fc',\n",
       " 'transformer.h.1.mlp.c_proj',\n",
       " 'transformer.h.1.mlp.act',\n",
       " 'transformer.h.1.mlp.dropout',\n",
       " 'transformer.h.2',\n",
       " 'transformer.h.2.ln_1',\n",
       " 'transformer.h.2.attn',\n",
       " 'transformer.h.2.attn.c_attn',\n",
       " 'transformer.h.2.attn.c_proj',\n",
       " 'transformer.h.2.attn.attn_dropout',\n",
       " 'transformer.h.2.attn.resid_dropout',\n",
       " 'transformer.h.2.ln_2',\n",
       " 'transformer.h.2.mlp',\n",
       " 'transformer.h.2.mlp.c_fc',\n",
       " 'transformer.h.2.mlp.c_proj',\n",
       " 'transformer.h.2.mlp.act',\n",
       " 'transformer.h.2.mlp.dropout',\n",
       " 'transformer.h.3',\n",
       " 'transformer.h.3.ln_1',\n",
       " 'transformer.h.3.attn',\n",
       " 'transformer.h.3.attn.c_attn',\n",
       " 'transformer.h.3.attn.c_proj',\n",
       " 'transformer.h.3.attn.attn_dropout',\n",
       " 'transformer.h.3.attn.resid_dropout',\n",
       " 'transformer.h.3.ln_2',\n",
       " 'transformer.h.3.mlp',\n",
       " 'transformer.h.3.mlp.c_fc',\n",
       " 'transformer.h.3.mlp.c_proj',\n",
       " 'transformer.h.3.mlp.act',\n",
       " 'transformer.h.3.mlp.dropout',\n",
       " 'transformer.h.4',\n",
       " 'transformer.h.4.ln_1',\n",
       " 'transformer.h.4.attn',\n",
       " 'transformer.h.4.attn.c_attn',\n",
       " 'transformer.h.4.attn.c_proj',\n",
       " 'transformer.h.4.attn.attn_dropout',\n",
       " 'transformer.h.4.attn.resid_dropout',\n",
       " 'transformer.h.4.ln_2',\n",
       " 'transformer.h.4.mlp',\n",
       " 'transformer.h.4.mlp.c_fc',\n",
       " 'transformer.h.4.mlp.c_proj',\n",
       " 'transformer.h.4.mlp.act',\n",
       " 'transformer.h.4.mlp.dropout',\n",
       " 'transformer.h.5',\n",
       " 'transformer.h.5.ln_1',\n",
       " 'transformer.h.5.attn',\n",
       " 'transformer.h.5.attn.c_attn',\n",
       " 'transformer.h.5.attn.c_proj',\n",
       " 'transformer.h.5.attn.attn_dropout',\n",
       " 'transformer.h.5.attn.resid_dropout',\n",
       " 'transformer.h.5.ln_2',\n",
       " 'transformer.h.5.mlp',\n",
       " 'transformer.h.5.mlp.c_fc',\n",
       " 'transformer.h.5.mlp.c_proj',\n",
       " 'transformer.h.5.mlp.act',\n",
       " 'transformer.h.5.mlp.dropout',\n",
       " 'transformer.h.6',\n",
       " 'transformer.h.6.ln_1',\n",
       " 'transformer.h.6.attn',\n",
       " 'transformer.h.6.attn.c_attn',\n",
       " 'transformer.h.6.attn.c_proj',\n",
       " 'transformer.h.6.attn.attn_dropout',\n",
       " 'transformer.h.6.attn.resid_dropout',\n",
       " 'transformer.h.6.ln_2',\n",
       " 'transformer.h.6.mlp',\n",
       " 'transformer.h.6.mlp.c_fc',\n",
       " 'transformer.h.6.mlp.c_proj',\n",
       " 'transformer.h.6.mlp.act',\n",
       " 'transformer.h.6.mlp.dropout',\n",
       " 'transformer.h.7',\n",
       " 'transformer.h.7.ln_1',\n",
       " 'transformer.h.7.attn',\n",
       " 'transformer.h.7.attn.c_attn',\n",
       " 'transformer.h.7.attn.c_proj',\n",
       " 'transformer.h.7.attn.attn_dropout',\n",
       " 'transformer.h.7.attn.resid_dropout',\n",
       " 'transformer.h.7.ln_2',\n",
       " 'transformer.h.7.mlp',\n",
       " 'transformer.h.7.mlp.c_fc',\n",
       " 'transformer.h.7.mlp.c_proj',\n",
       " 'transformer.h.7.mlp.act',\n",
       " 'transformer.h.7.mlp.dropout',\n",
       " 'transformer.h.8',\n",
       " 'transformer.h.8.ln_1',\n",
       " 'transformer.h.8.attn',\n",
       " 'transformer.h.8.attn.c_attn',\n",
       " 'transformer.h.8.attn.c_proj',\n",
       " 'transformer.h.8.attn.attn_dropout',\n",
       " 'transformer.h.8.attn.resid_dropout',\n",
       " 'transformer.h.8.ln_2',\n",
       " 'transformer.h.8.mlp',\n",
       " 'transformer.h.8.mlp.c_fc',\n",
       " 'transformer.h.8.mlp.c_proj',\n",
       " 'transformer.h.8.mlp.act',\n",
       " 'transformer.h.8.mlp.dropout',\n",
       " 'transformer.h.9',\n",
       " 'transformer.h.9.ln_1',\n",
       " 'transformer.h.9.attn',\n",
       " 'transformer.h.9.attn.c_attn',\n",
       " 'transformer.h.9.attn.c_proj',\n",
       " 'transformer.h.9.attn.attn_dropout',\n",
       " 'transformer.h.9.attn.resid_dropout',\n",
       " 'transformer.h.9.ln_2',\n",
       " 'transformer.h.9.mlp',\n",
       " 'transformer.h.9.mlp.c_fc',\n",
       " 'transformer.h.9.mlp.c_proj',\n",
       " 'transformer.h.9.mlp.act',\n",
       " 'transformer.h.9.mlp.dropout',\n",
       " 'transformer.h.10',\n",
       " 'transformer.h.10.ln_1',\n",
       " 'transformer.h.10.attn',\n",
       " 'transformer.h.10.attn.c_attn',\n",
       " 'transformer.h.10.attn.c_proj',\n",
       " 'transformer.h.10.attn.attn_dropout',\n",
       " 'transformer.h.10.attn.resid_dropout',\n",
       " 'transformer.h.10.ln_2',\n",
       " 'transformer.h.10.mlp',\n",
       " 'transformer.h.10.mlp.c_fc',\n",
       " 'transformer.h.10.mlp.c_proj',\n",
       " 'transformer.h.10.mlp.act',\n",
       " 'transformer.h.10.mlp.dropout',\n",
       " 'transformer.h.11',\n",
       " 'transformer.h.11.ln_1',\n",
       " 'transformer.h.11.attn',\n",
       " 'transformer.h.11.attn.c_attn',\n",
       " 'transformer.h.11.attn.c_proj',\n",
       " 'transformer.h.11.attn.attn_dropout',\n",
       " 'transformer.h.11.attn.resid_dropout',\n",
       " 'transformer.h.11.ln_2',\n",
       " 'transformer.h.11.mlp',\n",
       " 'transformer.h.11.mlp.c_fc',\n",
       " 'transformer.h.11.mlp.c_proj',\n",
       " 'transformer.h.11.mlp.act',\n",
       " 'transformer.h.11.mlp.dropout',\n",
       " 'transformer.h.12',\n",
       " 'transformer.h.12.ln_1',\n",
       " 'transformer.h.12.attn',\n",
       " 'transformer.h.12.attn.c_attn',\n",
       " 'transformer.h.12.attn.c_proj',\n",
       " 'transformer.h.12.attn.attn_dropout',\n",
       " 'transformer.h.12.attn.resid_dropout',\n",
       " 'transformer.h.12.ln_2',\n",
       " 'transformer.h.12.mlp',\n",
       " 'transformer.h.12.mlp.c_fc',\n",
       " 'transformer.h.12.mlp.c_proj',\n",
       " 'transformer.h.12.mlp.act',\n",
       " 'transformer.h.12.mlp.dropout',\n",
       " 'transformer.h.13',\n",
       " 'transformer.h.13.ln_1',\n",
       " 'transformer.h.13.attn',\n",
       " 'transformer.h.13.attn.c_attn',\n",
       " 'transformer.h.13.attn.c_proj',\n",
       " 'transformer.h.13.attn.attn_dropout',\n",
       " 'transformer.h.13.attn.resid_dropout',\n",
       " 'transformer.h.13.ln_2',\n",
       " 'transformer.h.13.mlp',\n",
       " 'transformer.h.13.mlp.c_fc',\n",
       " 'transformer.h.13.mlp.c_proj',\n",
       " 'transformer.h.13.mlp.act',\n",
       " 'transformer.h.13.mlp.dropout',\n",
       " 'transformer.h.14',\n",
       " 'transformer.h.14.ln_1',\n",
       " 'transformer.h.14.attn',\n",
       " 'transformer.h.14.attn.c_attn',\n",
       " 'transformer.h.14.attn.c_proj',\n",
       " 'transformer.h.14.attn.attn_dropout',\n",
       " 'transformer.h.14.attn.resid_dropout',\n",
       " 'transformer.h.14.ln_2',\n",
       " 'transformer.h.14.mlp',\n",
       " 'transformer.h.14.mlp.c_fc',\n",
       " 'transformer.h.14.mlp.c_proj',\n",
       " 'transformer.h.14.mlp.act',\n",
       " 'transformer.h.14.mlp.dropout',\n",
       " 'transformer.h.15',\n",
       " 'transformer.h.15.ln_1',\n",
       " 'transformer.h.15.attn',\n",
       " 'transformer.h.15.attn.c_attn',\n",
       " 'transformer.h.15.attn.c_proj',\n",
       " 'transformer.h.15.attn.attn_dropout',\n",
       " 'transformer.h.15.attn.resid_dropout',\n",
       " 'transformer.h.15.ln_2',\n",
       " 'transformer.h.15.mlp',\n",
       " 'transformer.h.15.mlp.c_fc',\n",
       " 'transformer.h.15.mlp.c_proj',\n",
       " 'transformer.h.15.mlp.act',\n",
       " 'transformer.h.15.mlp.dropout',\n",
       " 'transformer.h.16',\n",
       " 'transformer.h.16.ln_1',\n",
       " 'transformer.h.16.attn',\n",
       " 'transformer.h.16.attn.c_attn',\n",
       " 'transformer.h.16.attn.c_proj',\n",
       " 'transformer.h.16.attn.attn_dropout',\n",
       " 'transformer.h.16.attn.resid_dropout',\n",
       " 'transformer.h.16.ln_2',\n",
       " 'transformer.h.16.mlp',\n",
       " 'transformer.h.16.mlp.c_fc',\n",
       " 'transformer.h.16.mlp.c_proj',\n",
       " 'transformer.h.16.mlp.act',\n",
       " 'transformer.h.16.mlp.dropout',\n",
       " 'transformer.h.17',\n",
       " 'transformer.h.17.ln_1',\n",
       " 'transformer.h.17.attn',\n",
       " 'transformer.h.17.attn.c_attn',\n",
       " 'transformer.h.17.attn.c_proj',\n",
       " 'transformer.h.17.attn.attn_dropout',\n",
       " 'transformer.h.17.attn.resid_dropout',\n",
       " 'transformer.h.17.ln_2',\n",
       " 'transformer.h.17.mlp',\n",
       " 'transformer.h.17.mlp.c_fc',\n",
       " 'transformer.h.17.mlp.c_proj',\n",
       " 'transformer.h.17.mlp.act',\n",
       " 'transformer.h.17.mlp.dropout',\n",
       " 'transformer.h.18',\n",
       " 'transformer.h.18.ln_1',\n",
       " 'transformer.h.18.attn',\n",
       " 'transformer.h.18.attn.c_attn',\n",
       " 'transformer.h.18.attn.c_proj',\n",
       " 'transformer.h.18.attn.attn_dropout',\n",
       " 'transformer.h.18.attn.resid_dropout',\n",
       " 'transformer.h.18.ln_2',\n",
       " 'transformer.h.18.mlp',\n",
       " 'transformer.h.18.mlp.c_fc',\n",
       " 'transformer.h.18.mlp.c_proj',\n",
       " 'transformer.h.18.mlp.act',\n",
       " 'transformer.h.18.mlp.dropout',\n",
       " 'transformer.h.19',\n",
       " 'transformer.h.19.ln_1',\n",
       " 'transformer.h.19.attn',\n",
       " 'transformer.h.19.attn.c_attn',\n",
       " 'transformer.h.19.attn.c_proj',\n",
       " 'transformer.h.19.attn.attn_dropout',\n",
       " 'transformer.h.19.attn.resid_dropout',\n",
       " 'transformer.h.19.ln_2',\n",
       " 'transformer.h.19.mlp',\n",
       " 'transformer.h.19.mlp.c_fc',\n",
       " 'transformer.h.19.mlp.c_proj',\n",
       " 'transformer.h.19.mlp.act',\n",
       " 'transformer.h.19.mlp.dropout',\n",
       " 'transformer.h.20',\n",
       " 'transformer.h.20.ln_1',\n",
       " 'transformer.h.20.attn',\n",
       " 'transformer.h.20.attn.c_attn',\n",
       " 'transformer.h.20.attn.c_proj',\n",
       " 'transformer.h.20.attn.attn_dropout',\n",
       " 'transformer.h.20.attn.resid_dropout',\n",
       " 'transformer.h.20.ln_2',\n",
       " 'transformer.h.20.mlp',\n",
       " 'transformer.h.20.mlp.c_fc',\n",
       " 'transformer.h.20.mlp.c_proj',\n",
       " 'transformer.h.20.mlp.act',\n",
       " 'transformer.h.20.mlp.dropout',\n",
       " 'transformer.h.21',\n",
       " 'transformer.h.21.ln_1',\n",
       " 'transformer.h.21.attn',\n",
       " 'transformer.h.21.attn.c_attn',\n",
       " 'transformer.h.21.attn.c_proj',\n",
       " 'transformer.h.21.attn.attn_dropout',\n",
       " 'transformer.h.21.attn.resid_dropout',\n",
       " 'transformer.h.21.ln_2',\n",
       " 'transformer.h.21.mlp',\n",
       " 'transformer.h.21.mlp.c_fc',\n",
       " 'transformer.h.21.mlp.c_proj',\n",
       " 'transformer.h.21.mlp.act',\n",
       " 'transformer.h.21.mlp.dropout',\n",
       " 'transformer.h.22',\n",
       " 'transformer.h.22.ln_1',\n",
       " 'transformer.h.22.attn',\n",
       " 'transformer.h.22.attn.c_attn',\n",
       " 'transformer.h.22.attn.c_proj',\n",
       " 'transformer.h.22.attn.attn_dropout',\n",
       " 'transformer.h.22.attn.resid_dropout',\n",
       " 'transformer.h.22.ln_2',\n",
       " 'transformer.h.22.mlp',\n",
       " 'transformer.h.22.mlp.c_fc',\n",
       " 'transformer.h.22.mlp.c_proj',\n",
       " 'transformer.h.22.mlp.act',\n",
       " 'transformer.h.22.mlp.dropout',\n",
       " 'transformer.h.23',\n",
       " 'transformer.h.23.ln_1',\n",
       " 'transformer.h.23.attn',\n",
       " 'transformer.h.23.attn.c_attn',\n",
       " 'transformer.h.23.attn.c_proj',\n",
       " 'transformer.h.23.attn.attn_dropout',\n",
       " 'transformer.h.23.attn.resid_dropout',\n",
       " 'transformer.h.23.ln_2',\n",
       " 'transformer.h.23.mlp',\n",
       " 'transformer.h.23.mlp.c_fc',\n",
       " 'transformer.h.23.mlp.c_proj',\n",
       " 'transformer.h.23.mlp.act',\n",
       " 'transformer.h.23.mlp.dropout',\n",
       " 'transformer.h.24',\n",
       " 'transformer.h.24.ln_1',\n",
       " 'transformer.h.24.attn',\n",
       " 'transformer.h.24.attn.c_attn',\n",
       " 'transformer.h.24.attn.c_proj',\n",
       " 'transformer.h.24.attn.attn_dropout',\n",
       " 'transformer.h.24.attn.resid_dropout',\n",
       " 'transformer.h.24.ln_2',\n",
       " 'transformer.h.24.mlp',\n",
       " 'transformer.h.24.mlp.c_fc',\n",
       " 'transformer.h.24.mlp.c_proj',\n",
       " 'transformer.h.24.mlp.act',\n",
       " 'transformer.h.24.mlp.dropout',\n",
       " 'transformer.h.25',\n",
       " 'transformer.h.25.ln_1',\n",
       " 'transformer.h.25.attn',\n",
       " 'transformer.h.25.attn.c_attn',\n",
       " 'transformer.h.25.attn.c_proj',\n",
       " 'transformer.h.25.attn.attn_dropout',\n",
       " 'transformer.h.25.attn.resid_dropout',\n",
       " 'transformer.h.25.ln_2',\n",
       " 'transformer.h.25.mlp',\n",
       " 'transformer.h.25.mlp.c_fc',\n",
       " 'transformer.h.25.mlp.c_proj',\n",
       " 'transformer.h.25.mlp.act',\n",
       " 'transformer.h.25.mlp.dropout',\n",
       " 'transformer.h.26',\n",
       " 'transformer.h.26.ln_1',\n",
       " 'transformer.h.26.attn',\n",
       " 'transformer.h.26.attn.c_attn',\n",
       " 'transformer.h.26.attn.c_proj',\n",
       " 'transformer.h.26.attn.attn_dropout',\n",
       " 'transformer.h.26.attn.resid_dropout',\n",
       " 'transformer.h.26.ln_2',\n",
       " 'transformer.h.26.mlp',\n",
       " 'transformer.h.26.mlp.c_fc',\n",
       " 'transformer.h.26.mlp.c_proj',\n",
       " 'transformer.h.26.mlp.act',\n",
       " 'transformer.h.26.mlp.dropout',\n",
       " 'transformer.h.27',\n",
       " 'transformer.h.27.ln_1',\n",
       " 'transformer.h.27.attn',\n",
       " 'transformer.h.27.attn.c_attn',\n",
       " 'transformer.h.27.attn.c_proj',\n",
       " 'transformer.h.27.attn.attn_dropout',\n",
       " 'transformer.h.27.attn.resid_dropout',\n",
       " 'transformer.h.27.ln_2',\n",
       " 'transformer.h.27.mlp',\n",
       " 'transformer.h.27.mlp.c_fc',\n",
       " 'transformer.h.27.mlp.c_proj',\n",
       " 'transformer.h.27.mlp.act',\n",
       " 'transformer.h.27.mlp.dropout',\n",
       " 'transformer.h.28',\n",
       " 'transformer.h.28.ln_1',\n",
       " 'transformer.h.28.attn',\n",
       " 'transformer.h.28.attn.c_attn',\n",
       " 'transformer.h.28.attn.c_proj',\n",
       " 'transformer.h.28.attn.attn_dropout',\n",
       " 'transformer.h.28.attn.resid_dropout',\n",
       " 'transformer.h.28.ln_2',\n",
       " 'transformer.h.28.mlp',\n",
       " 'transformer.h.28.mlp.c_fc',\n",
       " 'transformer.h.28.mlp.c_proj',\n",
       " 'transformer.h.28.mlp.act',\n",
       " 'transformer.h.28.mlp.dropout',\n",
       " 'transformer.h.29',\n",
       " 'transformer.h.29.ln_1',\n",
       " 'transformer.h.29.attn',\n",
       " 'transformer.h.29.attn.c_attn',\n",
       " 'transformer.h.29.attn.c_proj',\n",
       " 'transformer.h.29.attn.attn_dropout',\n",
       " 'transformer.h.29.attn.resid_dropout',\n",
       " 'transformer.h.29.ln_2',\n",
       " 'transformer.h.29.mlp',\n",
       " 'transformer.h.29.mlp.c_fc',\n",
       " 'transformer.h.29.mlp.c_proj',\n",
       " 'transformer.h.29.mlp.act',\n",
       " 'transformer.h.29.mlp.dropout',\n",
       " 'transformer.h.30',\n",
       " 'transformer.h.30.ln_1',\n",
       " 'transformer.h.30.attn',\n",
       " 'transformer.h.30.attn.c_attn',\n",
       " 'transformer.h.30.attn.c_proj',\n",
       " 'transformer.h.30.attn.attn_dropout',\n",
       " 'transformer.h.30.attn.resid_dropout',\n",
       " 'transformer.h.30.ln_2',\n",
       " 'transformer.h.30.mlp',\n",
       " 'transformer.h.30.mlp.c_fc',\n",
       " 'transformer.h.30.mlp.c_proj',\n",
       " 'transformer.h.30.mlp.act',\n",
       " 'transformer.h.30.mlp.dropout',\n",
       " 'transformer.h.31',\n",
       " 'transformer.h.31.ln_1',\n",
       " 'transformer.h.31.attn',\n",
       " 'transformer.h.31.attn.c_attn',\n",
       " 'transformer.h.31.attn.c_proj',\n",
       " 'transformer.h.31.attn.attn_dropout',\n",
       " 'transformer.h.31.attn.resid_dropout',\n",
       " 'transformer.h.31.ln_2',\n",
       " 'transformer.h.31.mlp',\n",
       " 'transformer.h.31.mlp.c_fc',\n",
       " 'transformer.h.31.mlp.c_proj',\n",
       " 'transformer.h.31.mlp.act',\n",
       " 'transformer.h.31.mlp.dropout',\n",
       " 'transformer.h.32',\n",
       " 'transformer.h.32.ln_1',\n",
       " 'transformer.h.32.attn',\n",
       " 'transformer.h.32.attn.c_attn',\n",
       " 'transformer.h.32.attn.c_proj',\n",
       " 'transformer.h.32.attn.attn_dropout',\n",
       " 'transformer.h.32.attn.resid_dropout',\n",
       " 'transformer.h.32.ln_2',\n",
       " 'transformer.h.32.mlp',\n",
       " 'transformer.h.32.mlp.c_fc',\n",
       " 'transformer.h.32.mlp.c_proj',\n",
       " 'transformer.h.32.mlp.act',\n",
       " 'transformer.h.32.mlp.dropout',\n",
       " 'transformer.h.33',\n",
       " 'transformer.h.33.ln_1',\n",
       " 'transformer.h.33.attn',\n",
       " 'transformer.h.33.attn.c_attn',\n",
       " 'transformer.h.33.attn.c_proj',\n",
       " 'transformer.h.33.attn.attn_dropout',\n",
       " 'transformer.h.33.attn.resid_dropout',\n",
       " 'transformer.h.33.ln_2',\n",
       " 'transformer.h.33.mlp',\n",
       " 'transformer.h.33.mlp.c_fc',\n",
       " 'transformer.h.33.mlp.c_proj',\n",
       " 'transformer.h.33.mlp.act',\n",
       " 'transformer.h.33.mlp.dropout',\n",
       " 'transformer.h.34',\n",
       " 'transformer.h.34.ln_1',\n",
       " 'transformer.h.34.attn',\n",
       " 'transformer.h.34.attn.c_attn',\n",
       " 'transformer.h.34.attn.c_proj',\n",
       " 'transformer.h.34.attn.attn_dropout',\n",
       " 'transformer.h.34.attn.resid_dropout',\n",
       " 'transformer.h.34.ln_2',\n",
       " 'transformer.h.34.mlp',\n",
       " 'transformer.h.34.mlp.c_fc',\n",
       " 'transformer.h.34.mlp.c_proj',\n",
       " 'transformer.h.34.mlp.act',\n",
       " 'transformer.h.34.mlp.dropout',\n",
       " 'transformer.h.35',\n",
       " 'transformer.h.35.ln_1',\n",
       " 'transformer.h.35.attn',\n",
       " 'transformer.h.35.attn.c_attn',\n",
       " 'transformer.h.35.attn.c_proj',\n",
       " 'transformer.h.35.attn.attn_dropout',\n",
       " 'transformer.h.35.attn.resid_dropout',\n",
       " 'transformer.h.35.ln_2',\n",
       " 'transformer.h.35.mlp',\n",
       " 'transformer.h.35.mlp.c_fc',\n",
       " 'transformer.h.35.mlp.c_proj',\n",
       " 'transformer.h.35.mlp.act',\n",
       " 'transformer.h.35.mlp.dropout',\n",
       " 'transformer.h.36',\n",
       " 'transformer.h.36.ln_1',\n",
       " 'transformer.h.36.attn',\n",
       " 'transformer.h.36.attn.c_attn',\n",
       " 'transformer.h.36.attn.c_proj',\n",
       " 'transformer.h.36.attn.attn_dropout',\n",
       " 'transformer.h.36.attn.resid_dropout',\n",
       " 'transformer.h.36.ln_2',\n",
       " 'transformer.h.36.mlp',\n",
       " 'transformer.h.36.mlp.c_fc',\n",
       " 'transformer.h.36.mlp.c_proj',\n",
       " 'transformer.h.36.mlp.act',\n",
       " 'transformer.h.36.mlp.dropout',\n",
       " 'transformer.h.37',\n",
       " 'transformer.h.37.ln_1',\n",
       " 'transformer.h.37.attn',\n",
       " 'transformer.h.37.attn.c_attn',\n",
       " 'transformer.h.37.attn.c_proj',\n",
       " 'transformer.h.37.attn.attn_dropout',\n",
       " 'transformer.h.37.attn.resid_dropout',\n",
       " 'transformer.h.37.ln_2',\n",
       " 'transformer.h.37.mlp',\n",
       " 'transformer.h.37.mlp.c_fc',\n",
       " 'transformer.h.37.mlp.c_proj',\n",
       " 'transformer.h.37.mlp.act',\n",
       " 'transformer.h.37.mlp.dropout',\n",
       " 'transformer.h.38',\n",
       " 'transformer.h.38.ln_1',\n",
       " 'transformer.h.38.attn',\n",
       " 'transformer.h.38.attn.c_attn',\n",
       " 'transformer.h.38.attn.c_proj',\n",
       " 'transformer.h.38.attn.attn_dropout',\n",
       " 'transformer.h.38.attn.resid_dropout',\n",
       " 'transformer.h.38.ln_2',\n",
       " 'transformer.h.38.mlp',\n",
       " 'transformer.h.38.mlp.c_fc',\n",
       " 'transformer.h.38.mlp.c_proj',\n",
       " 'transformer.h.38.mlp.act',\n",
       " 'transformer.h.38.mlp.dropout',\n",
       " 'transformer.h.39',\n",
       " 'transformer.h.39.ln_1',\n",
       " 'transformer.h.39.attn',\n",
       " 'transformer.h.39.attn.c_attn',\n",
       " 'transformer.h.39.attn.c_proj',\n",
       " 'transformer.h.39.attn.attn_dropout',\n",
       " 'transformer.h.39.attn.resid_dropout',\n",
       " 'transformer.h.39.ln_2',\n",
       " 'transformer.h.39.mlp',\n",
       " 'transformer.h.39.mlp.c_fc',\n",
       " 'transformer.h.39.mlp.c_proj',\n",
       " 'transformer.h.39.mlp.act',\n",
       " 'transformer.h.39.mlp.dropout',\n",
       " 'transformer.h.40',\n",
       " 'transformer.h.40.ln_1',\n",
       " 'transformer.h.40.attn',\n",
       " 'transformer.h.40.attn.c_attn',\n",
       " 'transformer.h.40.attn.c_proj',\n",
       " 'transformer.h.40.attn.attn_dropout',\n",
       " 'transformer.h.40.attn.resid_dropout',\n",
       " 'transformer.h.40.ln_2',\n",
       " 'transformer.h.40.mlp',\n",
       " 'transformer.h.40.mlp.c_fc',\n",
       " 'transformer.h.40.mlp.c_proj',\n",
       " 'transformer.h.40.mlp.act',\n",
       " 'transformer.h.40.mlp.dropout',\n",
       " 'transformer.h.41',\n",
       " 'transformer.h.41.ln_1',\n",
       " 'transformer.h.41.attn',\n",
       " 'transformer.h.41.attn.c_attn',\n",
       " 'transformer.h.41.attn.c_proj',\n",
       " 'transformer.h.41.attn.attn_dropout',\n",
       " 'transformer.h.41.attn.resid_dropout',\n",
       " 'transformer.h.41.ln_2',\n",
       " 'transformer.h.41.mlp',\n",
       " 'transformer.h.41.mlp.c_fc',\n",
       " 'transformer.h.41.mlp.c_proj',\n",
       " 'transformer.h.41.mlp.act',\n",
       " 'transformer.h.41.mlp.dropout',\n",
       " 'transformer.h.42',\n",
       " 'transformer.h.42.ln_1',\n",
       " 'transformer.h.42.attn',\n",
       " 'transformer.h.42.attn.c_attn',\n",
       " 'transformer.h.42.attn.c_proj',\n",
       " 'transformer.h.42.attn.attn_dropout',\n",
       " 'transformer.h.42.attn.resid_dropout',\n",
       " 'transformer.h.42.ln_2',\n",
       " 'transformer.h.42.mlp',\n",
       " 'transformer.h.42.mlp.c_fc',\n",
       " 'transformer.h.42.mlp.c_proj',\n",
       " 'transformer.h.42.mlp.act',\n",
       " 'transformer.h.42.mlp.dropout',\n",
       " 'transformer.h.43',\n",
       " 'transformer.h.43.ln_1',\n",
       " 'transformer.h.43.attn',\n",
       " 'transformer.h.43.attn.c_attn',\n",
       " 'transformer.h.43.attn.c_proj',\n",
       " 'transformer.h.43.attn.attn_dropout',\n",
       " 'transformer.h.43.attn.resid_dropout',\n",
       " 'transformer.h.43.ln_2',\n",
       " 'transformer.h.43.mlp',\n",
       " 'transformer.h.43.mlp.c_fc',\n",
       " 'transformer.h.43.mlp.c_proj',\n",
       " 'transformer.h.43.mlp.act',\n",
       " 'transformer.h.43.mlp.dropout',\n",
       " 'transformer.h.44',\n",
       " 'transformer.h.44.ln_1',\n",
       " 'transformer.h.44.attn',\n",
       " 'transformer.h.44.attn.c_attn',\n",
       " 'transformer.h.44.attn.c_proj',\n",
       " 'transformer.h.44.attn.attn_dropout',\n",
       " 'transformer.h.44.attn.resid_dropout',\n",
       " 'transformer.h.44.ln_2',\n",
       " 'transformer.h.44.mlp',\n",
       " 'transformer.h.44.mlp.c_fc',\n",
       " 'transformer.h.44.mlp.c_proj',\n",
       " 'transformer.h.44.mlp.act',\n",
       " 'transformer.h.44.mlp.dropout',\n",
       " 'transformer.h.45',\n",
       " 'transformer.h.45.ln_1',\n",
       " 'transformer.h.45.attn',\n",
       " 'transformer.h.45.attn.c_attn',\n",
       " 'transformer.h.45.attn.c_proj',\n",
       " 'transformer.h.45.attn.attn_dropout',\n",
       " 'transformer.h.45.attn.resid_dropout',\n",
       " 'transformer.h.45.ln_2',\n",
       " 'transformer.h.45.mlp',\n",
       " 'transformer.h.45.mlp.c_fc',\n",
       " 'transformer.h.45.mlp.c_proj',\n",
       " 'transformer.h.45.mlp.act',\n",
       " 'transformer.h.45.mlp.dropout',\n",
       " 'transformer.h.46',\n",
       " 'transformer.h.46.ln_1',\n",
       " 'transformer.h.46.attn',\n",
       " 'transformer.h.46.attn.c_attn',\n",
       " 'transformer.h.46.attn.c_proj',\n",
       " 'transformer.h.46.attn.attn_dropout',\n",
       " 'transformer.h.46.attn.resid_dropout',\n",
       " 'transformer.h.46.ln_2',\n",
       " 'transformer.h.46.mlp',\n",
       " 'transformer.h.46.mlp.c_fc',\n",
       " 'transformer.h.46.mlp.c_proj',\n",
       " 'transformer.h.46.mlp.act',\n",
       " 'transformer.h.46.mlp.dropout',\n",
       " 'transformer.h.47',\n",
       " 'transformer.h.47.ln_1',\n",
       " 'transformer.h.47.attn',\n",
       " 'transformer.h.47.attn.c_attn',\n",
       " 'transformer.h.47.attn.c_proj',\n",
       " 'transformer.h.47.attn.attn_dropout',\n",
       " 'transformer.h.47.attn.resid_dropout',\n",
       " 'transformer.h.47.ln_2',\n",
       " 'transformer.h.47.mlp',\n",
       " 'transformer.h.47.mlp.c_fc',\n",
       " 'transformer.h.47.mlp.c_proj',\n",
       " 'transformer.h.47.mlp.act',\n",
       " 'transformer.h.47.mlp.dropout',\n",
       " 'transformer.ln_f',\n",
       " 'lm_head']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for (name, module) in model.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "xx = [(m[1], \"weight\") for m in filter(lambda nm: hasattr(nm[1], \"weight\") and \"mlp\" in nm[0], model.named_modules())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.66 GiB (GPU 0; 39.59 GiB total capacity; 26.08 GiB already allocated; 2.37 GiB free; 36.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9d9578014162>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mparameters_to_prune\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpruning_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1Unstructured\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mamount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/llm/lib/python3.7/site-packages/torch/nn/utils/prune.py\u001b[0m in \u001b[0;36mglobal_unstructured\u001b[0;34m(parameters, pruning_method, importance_scores, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[0;31m# use the `compute_mask` method from `PruningContainer` to combine the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[0;31m# mask computed by the new method with the pre-existing mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m     \u001b[0mfinal_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_importance_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;31m# Pointer for slicing the mask to match the shape of each parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/llm/lib/python3.7/site-packages/torch/nn/utils/prune.py\u001b[0m in \u001b[0;36mcompute_mask\u001b[0;34m(self, t, default_mask)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pruning_methods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_combine_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/llm/lib/python3.7/site-packages/torch/nn/utils/prune.py\u001b[0m in \u001b[0;36m_combine_masks\u001b[0;34m(method, t, mask)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;31m# compute the new mask on the unpruned slice of the tensor t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0mpartial_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m             \u001b[0mnew_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.66 GiB (GPU 0; 39.59 GiB total capacity; 26.08 GiB already allocated; 2.37 GiB free; 36.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 6            |        cudaMalloc retries: 8         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   31322 MB |   39610 MB |   18772 GB |   18741 GB |\n",
      "|       from large pool |   31315 MB |   39547 MB |   18639 GB |   18609 GB |\n",
      "|       from small pool |       7 MB |     108 MB |     132 GB |     132 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   31322 MB |   39610 MB |   18772 GB |   18741 GB |\n",
      "|       from large pool |   31315 MB |   39547 MB |   18639 GB |   18609 GB |\n",
      "|       from small pool |       7 MB |     108 MB |     132 GB |     132 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   35596 MB |   39624 MB |   67718 MB |   32122 MB |\n",
      "|       from large pool |   35524 MB |   39560 MB |   67604 MB |   32080 MB |\n",
      "|       from small pool |      72 MB |     110 MB |     114 MB |      42 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    4225 MB |   18405 MB |   18661 GB |   18657 GB |\n",
      "|       from large pool |    4208 MB |   18399 MB |   18529 GB |   18525 GB |\n",
      "|       from small pool |      16 MB |      18 MB |     132 GB |     132 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1584    |    2549    |     903 K  |     901 K  |\n",
      "|       from large pool |     441    |     634    |     598 K  |     597 K  |\n",
      "|       from small pool |    1143    |    1915    |     304 K  |     303 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1584    |    2549    |     903 K  |     901 K  |\n",
      "|       from large pool |     441    |     634    |     598 K  |     597 K  |\n",
      "|       from small pool |    1143    |    1915    |     304 K  |     303 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      57    |     371    |     477    |     420    |\n",
      "|       from large pool |      21    |     344    |     420    |     399    |\n",
      "|       from small pool |      36    |      55    |      57    |      21    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     211    |     214    |  440844    |  440633    |\n",
      "|       from large pool |      95    |      96    |  269917    |  269822    |\n",
      "|       from small pool |     116    |     118    |  170927    |  170811    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# del model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\").to(device)\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.7911, device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/christopherkang/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "Loading cached shuffled indices for dataset at /home/christopherkang/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-b23cfeb68a931a8d.arrow\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"imdb\", split=\"test\").shuffle(seed=42).select(range(1000))\n",
    "task_evaluator = evaluator(\"text-classification\")\n",
    "\n",
    "MODEL = \"gpt2-xl\"\n",
    "\n",
    "tokenizer, model = produce_model(MODEL)\n",
    "\n",
    "## pruning\n",
    "parameters_to_prune = [(m, \"weight\") for m in filter(lambda m: hasattr(m, \"weight\"), model.modules())]\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=sparsity,\n",
    ")\n",
    "\n",
    "## evaluation\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "print(generator(\"My name is Julien and I like to\", ))\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"lvwerra/distilbert-imdb\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\")\n",
    "\n",
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=data,\n",
    "    label_mapping={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    ")\n",
    "\n",
    "del tokenizer\n",
    "del model\n",
    "del parameters_to_prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/home/christopherkang/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "Loading cached shuffled indices for dataset at /home/christopherkang/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-c13da8c69afb5b41.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2ForQuestionAnswering: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Parameter 'function'=<function QuestionAnsweringEvaluator.is_squad_v2_format.<locals>.<lambda> at 0x146d1b700c20> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "`squad_v2_format` parameter not provided to QuestionAnsweringEvaluator.compute(). Automatically inferred `squad_v2_format` as False.\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"squad\", split=\"validation\").shuffle(seed=42).select(range(50))\n",
    "task_evaluator = evaluator(\"question-answering\")\n",
    "\n",
    "# MODEL = \"gpt2-xl\"\n",
    "\n",
    "# tokenizer, model = produce_model(MODEL)\n",
    "\n",
    "# ## pruning\n",
    "# parameters_to_prune = [(m, \"weight\") for m in filter(lambda m: hasattr(m, \"weight\"), model.modules())]\n",
    "\n",
    "# prune.global_unstructured(\n",
    "#     parameters_to_prune,\n",
    "#     pruning_method=prune.L1Unstructured,\n",
    "#     amount=sparsity,\n",
    "# )\n",
    "\n",
    "# ## evaluation\n",
    "# generator = pipeline(task=\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# print(generator(\"My name is Julien and I like to\", ))\n",
    "\n",
    "\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "# model = DebertaV2Model.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "model = DebertaV2ForQuestionAnswering.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "\n",
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=data\n",
    ")\n",
    "\n",
    "del tokenizer\n",
    "del model\n",
    "# del parameters_to_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.0,\n",
       " 'f1': 6.4492063492063485,\n",
       " 'total_time_in_seconds': 13.478586913988693,\n",
       " 'samples_per_second': 3.7095876829719976,\n",
       " 'latency_in_seconds': 0.2695717382797738}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-7406dc6b362d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdebert_tok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/llm/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3438\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3439\u001b[0m             \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3440\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3441\u001b[0m         )\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/llm/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_use_source_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_source_tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m         \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;31m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/llm/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mskip_special_tokens\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "## sample code from Madison\n",
    "debert = DebertaV2Model.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "debert_tok = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "inputs = debert_tok(\"A step by step recipe to make bolognese pasta:\", return_tensors=\"pt\")\n",
    "outputs = debert(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "debert_tok.decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/christopherkang/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "Loading cached shuffled indices for dataset at /home/christopherkang/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-b23cfeb68a931a8d.arrow\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-9ecacd920f8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel_or_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mlabel_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"NEGATIVE\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"POSITIVE\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/llm/lib/python3.7/site-packages/evaluate/evaluator/text_classification.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, second_input_column, label_column, label_mapping)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/llm/lib/python3.7/site-packages/evaluate/evaluator/base.py\u001b[0m in \u001b[0;36mprepare_pipeline\u001b[0;34m(self, model_or_pipeline, tokenizer, feature_extractor, device)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             )\n\u001b[1;32m    408\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/llm/lib/python3.7/site-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m                 \u001b[0;31m# Impossible to guess what is the right tokenizer here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                 raise Exception(\n\u001b[0;32m--> 786\u001b[0;31m                     \u001b[0;34m\"Impossible to guess which tokenizer to use. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m                     \u001b[0;34m\"Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 )\n",
      "\u001b[0;31mException\u001b[0m: Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.6520, -5.2399, -0.7639,  ..., -0.3622,  1.1090, -0.5299],\n",
       "         [-0.5814,  1.3496,  0.2630,  ...,  0.1930,  1.5334, -0.6259],\n",
       "         [-0.2318,  2.7247, -0.2569,  ...,  0.4096,  1.1944,  0.3863],\n",
       "         ...,\n",
       "         [-0.1746, -0.1909, -0.1213,  ..., -0.0068,  0.2581,  0.5316],\n",
       "         [-0.8192, -1.7572,  0.2999,  ...,  0.9059, -0.2985, -0.5376],\n",
       "         [-1.1186, -4.5928, -0.8893,  ..., -0.1412,  1.2751, -0.4949]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.0,\n",
       " 'f1': 7.0,\n",
       " 'total_time_in_seconds': 22.872378915082663,\n",
       " 'samples_per_second': 0.43720856659145896,\n",
       " 'latency_in_seconds': 2.287237891508266}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_MODELS = [\"gpt2-xl\", \"microsoft/deberta-v2-xxlarge\", \"facebook/m2m100_418M\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 258/258 [00:00<00:00, 244kB/s]\n",
      "Downloading: 100%|██████████| 798k/798k [00:00<00:00, 11.1MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 5.43MB/s]\n",
      "Downloading: 100%|██████████| 2.11M/2.11M [00:00<00:00, 15.6MB/s]\n",
      "Downloading: 100%|██████████| 99.0/99.0 [00:00<00:00, 98.6kB/s]\n",
      "Downloading: 100%|██████████| 975/975 [00:00<00:00, 401kB/s]\n",
      "Downloading: 100%|██████████| 75.5k/75.5k [00:00<00:00, 2.28MB/s]\n",
      "Downloading: 100%|██████████| 10.0G/10.0G [02:07<00:00, 78.7MB/s]\n",
      "Downloading: 100%|██████████| 3.15G/3.15G [00:41<00:00, 76.0MB/s]\n",
      "Some weights of the model checkpoint at ctkang/b_gpt2-xl_10 were not used when initializing GPT2LMHeadModel: ['transformer.h.37.mlp.c_fc.weight_orig', 'transformer.h.32.attn.c_proj.weight_orig', 'transformer.h.3.attn.c_attn.weight_mask', 'transformer.h.35.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_fc.weight_orig', 'transformer.h.20.ln_1.weight_mask', 'transformer.h.1.mlp.c_fc.weight_mask', 'transformer.h.10.attn.c_proj.weight_orig', 'transformer.h.33.mlp.c_proj.weight_mask', 'transformer.h.40.ln_2.weight_orig', 'transformer.h.34.mlp.c_proj.weight_mask', 'transformer.h.25.ln_2.weight_mask', 'transformer.h.11.attn.c_proj.weight_orig', 'transformer.h.16.ln_1.weight_mask', 'transformer.h.29.attn.c_proj.weight_orig', 'transformer.h.45.mlp.c_fc.weight_orig', 'transformer.h.36.attn.c_proj.weight_orig', 'transformer.h.28.mlp.c_fc.weight_mask', 'transformer.h.27.mlp.c_fc.weight_orig', 'transformer.h.44.mlp.c_fc.weight_orig', 'transformer.h.31.ln_2.weight_mask', 'transformer.h.17.ln_2.weight_orig', 'transformer.h.43.ln_1.weight_orig', 'transformer.h.6.attn.c_proj.weight_mask', 'transformer.h.20.mlp.c_proj.weight_mask', 'transformer.h.42.mlp.c_proj.weight_orig', 'transformer.h.19.mlp.c_fc.weight_mask', 'transformer.h.6.ln_2.weight_orig', 'transformer.h.1.ln_1.weight_orig', 'transformer.h.28.ln_1.weight_mask', 'transformer.h.38.mlp.c_proj.weight_orig', 'transformer.h.30.ln_2.weight_orig', 'transformer.h.10.ln_1.weight_mask', 'transformer.h.22.ln_2.weight_orig', 'transformer.h.27.attn.c_proj.weight_orig', 'transformer.h.13.attn.c_attn.weight_orig', 'transformer.h.16.ln_2.weight_orig', 'transformer.h.36.mlp.c_proj.weight_mask', 'transformer.h.17.ln_1.weight_orig', 'transformer.h.22.mlp.c_fc.weight_orig', 'transformer.h.2.attn.c_attn.weight_mask', 'transformer.h.10.mlp.c_fc.weight_mask', 'transformer.h.30.attn.c_proj.weight_orig', 'transformer.h.20.attn.c_proj.weight_mask', 'transformer.h.44.attn.c_proj.weight_mask', 'transformer.h.11.mlp.c_proj.weight_mask', 'transformer.h.6.ln_1.weight_mask', 'transformer.h.15.attn.c_proj.weight_mask', 'transformer.h.44.ln_1.weight_orig', 'transformer.h.41.ln_1.weight_orig', 'transformer.h.17.ln_2.weight_mask', 'transformer.h.32.mlp.c_fc.weight_orig', 'transformer.h.22.ln_1.weight_mask', 'transformer.h.3.ln_1.weight_mask', 'transformer.h.8.ln_1.weight_mask', 'transformer.h.12.mlp.c_fc.weight_orig', 'transformer.h.3.attn.c_proj.weight_mask', 'transformer.h.47.attn.c_proj.weight_orig', 'transformer.h.45.attn.c_attn.weight_mask', 'transformer.h.26.attn.c_attn.weight_mask', 'transformer.h.20.ln_2.weight_mask', 'transformer.h.46.ln_1.weight_mask', 'transformer.h.47.ln_2.weight_orig', 'transformer.h.38.attn.c_attn.weight_orig', 'transformer.h.6.attn.c_attn.weight_orig', 'transformer.h.25.mlp.c_proj.weight_orig', 'transformer.h.28.attn.c_attn.weight_orig', 'transformer.h.5.attn.c_proj.weight_mask', 'transformer.h.5.attn.c_attn.weight_orig', 'transformer.wte.weight_orig', 'transformer.h.9.ln_1.weight_orig', 'transformer.h.12.ln_2.weight_mask', 'transformer.h.35.ln_1.weight_orig', 'transformer.h.30.mlp.c_proj.weight_mask', 'transformer.h.42.ln_2.weight_orig', 'transformer.h.44.ln_2.weight_mask', 'transformer.h.36.attn.c_attn.weight_orig', 'transformer.h.2.attn.c_proj.weight_orig', 'transformer.h.32.ln_1.weight_mask', 'transformer.h.9.ln_2.weight_orig', 'transformer.h.21.attn.c_proj.weight_orig', 'transformer.h.7.ln_2.weight_orig', 'transformer.h.24.attn.c_attn.weight_orig', 'transformer.h.44.attn.c_proj.weight_orig', 'transformer.h.36.attn.c_proj.weight_mask', 'transformer.h.1.mlp.c_proj.weight_orig', 'transformer.h.42.attn.c_attn.weight_mask', 'transformer.h.12.attn.c_attn.weight_mask', 'transformer.h.6.ln_2.weight_mask', 'transformer.h.29.attn.c_proj.weight_mask', 'transformer.h.40.attn.c_proj.weight_mask', 'transformer.h.0.ln_2.weight_mask', 'transformer.h.17.attn.c_proj.weight_mask', 'transformer.h.33.ln_2.weight_mask', 'transformer.h.37.ln_2.weight_mask', 'transformer.h.18.ln_1.weight_orig', 'transformer.h.6.mlp.c_proj.weight_mask', 'transformer.h.29.mlp.c_fc.weight_orig', 'transformer.h.4.mlp.c_fc.weight_mask', 'transformer.h.24.mlp.c_proj.weight_mask', 'transformer.h.42.ln_2.weight_mask', 'transformer.h.38.attn.c_proj.weight_mask', 'transformer.h.30.ln_1.weight_orig', 'transformer.h.36.ln_2.weight_mask', 'transformer.h.12.mlp.c_proj.weight_mask', 'transformer.h.22.attn.c_proj.weight_mask', 'transformer.h.32.attn.c_attn.weight_mask', 'transformer.h.39.ln_2.weight_orig', 'transformer.h.4.ln_1.weight_orig', 'transformer.h.14.mlp.c_proj.weight_orig', 'transformer.h.23.attn.c_proj.weight_orig', 'transformer.h.27.attn.c_proj.weight_mask', 'transformer.h.13.mlp.c_fc.weight_mask', 'transformer.h.28.mlp.c_proj.weight_mask', 'transformer.h.14.ln_2.weight_mask', 'transformer.h.7.mlp.c_proj.weight_orig', 'transformer.h.19.ln_1.weight_mask', 'transformer.h.32.attn.c_attn.weight_orig', 'transformer.h.29.ln_1.weight_orig', 'transformer.h.39.ln_2.weight_mask', 'transformer.h.42.attn.c_attn.weight_orig', 'transformer.h.13.ln_1.weight_mask', 'transformer.h.15.attn.c_attn.weight_mask', 'transformer.h.7.ln_1.weight_orig', 'transformer.h.30.ln_2.weight_mask', 'transformer.h.9.ln_1.weight_mask', 'transformer.h.15.attn.c_proj.weight_orig', 'transformer.h.25.ln_1.weight_orig', 'transformer.h.31.attn.c_proj.weight_mask', 'transformer.h.10.ln_2.weight_mask', 'transformer.h.34.ln_2.weight_mask', 'transformer.h.20.ln_2.weight_orig', 'transformer.h.43.ln_1.weight_mask', 'transformer.h.6.mlp.c_proj.weight_orig', 'transformer.h.21.attn.c_attn.weight_orig', 'transformer.h.16.attn.c_proj.weight_orig', 'transformer.h.19.attn.c_attn.weight_orig', 'transformer.h.26.ln_1.weight_mask', 'transformer.h.28.mlp.c_proj.weight_orig', 'transformer.h.39.mlp.c_proj.weight_orig', 'transformer.h.45.mlp.c_proj.weight_mask', 'transformer.h.6.attn.c_attn.weight_mask', 'transformer.h.24.ln_2.weight_orig', 'transformer.h.7.attn.c_proj.weight_mask', 'transformer.h.39.ln_1.weight_mask', 'transformer.h.8.mlp.c_proj.weight_mask', 'transformer.h.6.ln_1.weight_orig', 'transformer.h.5.mlp.c_proj.weight_orig', 'transformer.h.0.mlp.c_fc.weight_mask', 'transformer.h.13.ln_2.weight_orig', 'transformer.h.16.attn.c_proj.weight_mask', 'transformer.h.33.mlp.c_proj.weight_orig', 'transformer.h.31.mlp.c_proj.weight_mask', 'transformer.h.43.attn.c_proj.weight_orig', 'transformer.h.24.mlp.c_fc.weight_orig', 'transformer.h.2.mlp.c_proj.weight_mask', 'transformer.h.21.attn.c_proj.weight_mask', 'transformer.h.30.ln_1.weight_mask', 'transformer.h.12.ln_1.weight_orig', 'transformer.h.11.attn.c_attn.weight_orig', 'transformer.h.0.mlp.c_proj.weight_mask', 'transformer.h.17.attn.c_proj.weight_orig', 'transformer.h.3.ln_2.weight_mask', 'transformer.h.45.mlp.c_proj.weight_orig', 'transformer.h.1.attn.c_proj.weight_orig', 'transformer.h.38.ln_2.weight_orig', 'transformer.h.37.attn.c_proj.weight_orig', 'transformer.h.12.mlp.c_fc.weight_mask', 'transformer.h.23.ln_1.weight_mask', 'transformer.h.42.mlp.c_fc.weight_mask', 'transformer.h.11.mlp.c_proj.weight_orig', 'transformer.h.20.attn.c_proj.weight_orig', 'transformer.h.17.mlp.c_proj.weight_mask', 'transformer.h.18.attn.c_attn.weight_orig', 'transformer.h.35.attn.c_proj.weight_mask', 'transformer.h.44.mlp.c_proj.weight_orig', 'transformer.h.21.ln_2.weight_mask', 'transformer.h.16.ln_1.weight_orig', 'transformer.h.16.ln_2.weight_mask', 'transformer.h.33.attn.c_attn.weight_mask', 'transformer.h.22.mlp.c_proj.weight_mask', 'transformer.h.14.mlp.c_fc.weight_orig', 'transformer.h.35.attn.c_attn.weight_orig', 'transformer.h.39.mlp.c_proj.weight_mask', 'transformer.h.7.ln_2.weight_mask', 'transformer.h.19.mlp.c_proj.weight_mask', 'transformer.h.0.attn.c_proj.weight_orig', 'transformer.h.9.mlp.c_proj.weight_orig', 'transformer.h.47.mlp.c_proj.weight_orig', 'transformer.h.41.attn.c_attn.weight_orig', 'transformer.h.32.mlp.c_fc.weight_mask', 'transformer.h.0.attn.c_attn.weight_orig', 'transformer.h.19.ln_1.weight_orig', 'transformer.h.1.attn.c_attn.weight_mask', 'transformer.h.38.ln_1.weight_mask', 'transformer.h.25.attn.c_attn.weight_orig', 'transformer.h.21.attn.c_attn.weight_mask', 'transformer.h.29.mlp.c_fc.weight_mask', 'transformer.h.21.ln_2.weight_orig', 'transformer.h.0.mlp.c_proj.weight_orig', 'transformer.h.35.mlp.c_proj.weight_mask', 'transformer.h.14.attn.c_attn.weight_mask', 'transformer.h.24.attn.c_proj.weight_orig', 'transformer.h.39.attn.c_proj.weight_mask', 'transformer.h.20.attn.c_attn.weight_orig', 'transformer.h.23.attn.c_attn.weight_mask', 'transformer.h.16.mlp.c_fc.weight_orig', 'transformer.h.30.attn.c_attn.weight_mask', 'transformer.h.9.attn.c_proj.weight_orig', 'transformer.h.14.attn.c_proj.weight_orig', 'transformer.h.17.mlp.c_fc.weight_orig', 'transformer.h.37.mlp.c_proj.weight_orig', 'transformer.h.0.mlp.c_fc.weight_orig', 'transformer.h.7.attn.c_attn.weight_orig', 'transformer.h.40.attn.c_proj.weight_orig', 'transformer.h.40.mlp.c_proj.weight_orig', 'transformer.h.36.mlp.c_fc.weight_orig', 'transformer.h.43.mlp.c_proj.weight_mask', 'transformer.h.41.attn.c_proj.weight_orig', 'transformer.h.10.mlp.c_fc.weight_orig', 'transformer.h.3.ln_1.weight_orig', 'transformer.h.47.ln_1.weight_mask', 'transformer.h.8.ln_2.weight_mask', 'transformer.h.28.ln_1.weight_orig', 'transformer.h.33.ln_2.weight_orig', 'transformer.h.36.attn.c_attn.weight_mask', 'transformer.h.41.mlp.c_proj.weight_orig', 'transformer.h.46.attn.c_attn.weight_mask', 'transformer.h.25.mlp.c_fc.weight_orig', 'transformer.h.15.attn.c_attn.weight_orig', 'transformer.h.9.mlp.c_fc.weight_mask', 'transformer.h.6.mlp.c_fc.weight_orig', 'transformer.h.21.mlp.c_fc.weight_orig', 'transformer.h.15.ln_1.weight_orig', 'transformer.h.5.mlp.c_proj.weight_mask', 'transformer.h.41.mlp.c_fc.weight_orig', 'transformer.h.42.mlp.c_fc.weight_orig', 'transformer.h.26.mlp.c_proj.weight_orig', 'transformer.h.47.attn.c_attn.weight_orig', 'transformer.h.22.attn.c_attn.weight_mask', 'transformer.h.29.ln_1.weight_mask', 'transformer.h.4.attn.c_proj.weight_mask', 'transformer.h.27.ln_1.weight_orig', 'transformer.h.26.attn.c_attn.weight_orig', 'transformer.h.2.ln_1.weight_orig', 'transformer.h.38.attn.c_proj.weight_orig', 'transformer.h.43.ln_2.weight_mask', 'transformer.h.38.mlp.c_proj.weight_mask', 'transformer.h.13.ln_1.weight_orig', 'transformer.h.21.mlp.c_proj.weight_mask', 'transformer.h.3.mlp.c_proj.weight_orig', 'transformer.h.33.ln_1.weight_mask', 'transformer.h.24.attn.c_proj.weight_mask', 'transformer.h.47.attn.c_proj.weight_mask', 'transformer.h.43.attn.c_attn.weight_orig', 'transformer.h.25.attn.c_proj.weight_orig', 'transformer.h.36.ln_2.weight_orig', 'transformer.h.33.mlp.c_fc.weight_mask', 'transformer.h.9.attn.c_attn.weight_mask', 'transformer.h.4.mlp.c_proj.weight_mask', 'transformer.h.3.mlp.c_fc.weight_orig', 'transformer.h.8.attn.c_attn.weight_mask', 'transformer.h.41.ln_2.weight_mask', 'transformer.h.17.attn.c_attn.weight_orig', 'transformer.h.44.mlp.c_fc.weight_mask', 'transformer.h.25.ln_2.weight_orig', 'transformer.h.34.ln_2.weight_orig', 'transformer.h.6.attn.c_proj.weight_orig', 'transformer.h.2.mlp.c_proj.weight_orig', 'transformer.h.36.mlp.c_proj.weight_orig', 'transformer.h.20.mlp.c_fc.weight_orig', 'transformer.h.15.mlp.c_proj.weight_orig', 'transformer.h.19.mlp.c_fc.weight_orig', 'transformer.h.26.mlp.c_fc.weight_orig', 'transformer.h.10.ln_1.weight_orig', 'transformer.h.37.attn.c_proj.weight_mask', 'transformer.h.46.mlp.c_fc.weight_mask', 'transformer.h.15.mlp.c_fc.weight_mask', 'transformer.h.26.ln_1.weight_orig', 'transformer.h.4.mlp.c_proj.weight_orig', 'transformer.h.9.attn.c_attn.weight_orig', 'transformer.h.26.attn.c_proj.weight_mask', 'transformer.h.4.ln_2.weight_mask', 'transformer.h.3.ln_2.weight_orig', 'transformer.h.45.ln_1.weight_orig', 'transformer.h.11.ln_1.weight_mask', 'transformer.h.8.attn.c_attn.weight_orig', 'transformer.h.40.mlp.c_proj.weight_mask', 'transformer.h.23.mlp.c_fc.weight_mask', 'transformer.h.25.ln_1.weight_mask', 'transformer.h.28.mlp.c_fc.weight_orig', 'transformer.h.8.attn.c_proj.weight_mask', 'transformer.h.23.attn.c_attn.weight_orig', 'transformer.h.27.ln_2.weight_orig', 'transformer.h.29.attn.c_attn.weight_mask', 'transformer.h.40.mlp.c_fc.weight_mask', 'transformer.h.14.ln_1.weight_orig', 'transformer.h.27.mlp.c_fc.weight_mask', 'transformer.h.47.mlp.c_proj.weight_mask', 'transformer.wpe.weight_orig', 'transformer.h.27.mlp.c_proj.weight_mask', 'transformer.h.34.attn.c_attn.weight_mask', 'transformer.h.24.ln_1.weight_orig', 'transformer.h.43.attn.c_proj.weight_mask', 'transformer.h.21.ln_1.weight_mask', 'transformer.h.41.mlp.c_fc.weight_mask', 'transformer.h.46.ln_2.weight_mask', 'transformer.h.34.mlp.c_fc.weight_mask', 'transformer.h.13.attn.c_proj.weight_orig', 'transformer.h.18.mlp.c_proj.weight_mask', 'transformer.ln_f.weight_orig', 'transformer.h.35.mlp.c_fc.weight_orig', 'transformer.h.23.mlp.c_proj.weight_orig', 'transformer.h.15.ln_1.weight_mask', 'transformer.h.23.attn.c_proj.weight_mask', 'transformer.h.26.ln_2.weight_mask', 'transformer.h.7.mlp.c_proj.weight_mask', 'transformer.h.18.ln_1.weight_mask', 'transformer.h.36.ln_1.weight_mask', 'transformer.h.2.mlp.c_fc.weight_orig', 'transformer.h.12.attn.c_proj.weight_mask', 'transformer.h.11.ln_2.weight_orig', 'transformer.h.44.attn.c_attn.weight_orig', 'transformer.h.34.attn.c_proj.weight_orig', 'transformer.ln_f.weight_mask', 'transformer.h.17.ln_1.weight_mask', 'transformer.h.45.ln_2.weight_orig', 'transformer.h.5.ln_1.weight_mask', 'transformer.h.13.attn.c_attn.weight_mask', 'transformer.h.40.ln_2.weight_mask', 'transformer.h.47.ln_2.weight_mask', 'transformer.h.39.attn.c_attn.weight_orig', 'transformer.h.32.mlp.c_proj.weight_orig', 'transformer.h.46.mlp.c_fc.weight_orig', 'transformer.h.8.ln_1.weight_orig', 'transformer.h.22.mlp.c_proj.weight_orig', 'transformer.h.1.mlp.c_proj.weight_mask', 'transformer.h.14.ln_1.weight_mask', 'transformer.h.22.attn.c_proj.weight_orig', 'transformer.h.45.attn.c_proj.weight_orig', 'transformer.h.7.attn.c_proj.weight_orig', 'transformer.h.35.mlp.c_proj.weight_orig', 'transformer.h.4.attn.c_proj.weight_orig', 'transformer.h.19.mlp.c_proj.weight_orig', 'transformer.h.31.mlp.c_fc.weight_orig', 'transformer.h.34.mlp.c_proj.weight_orig', 'transformer.h.20.ln_1.weight_orig', 'transformer.h.28.attn.c_proj.weight_mask', 'transformer.h.33.attn.c_proj.weight_mask', 'transformer.h.33.attn.c_proj.weight_orig', 'transformer.h.27.ln_2.weight_mask', 'transformer.h.44.attn.c_attn.weight_mask', 'transformer.h.40.mlp.c_fc.weight_orig', 'transformer.h.46.attn.c_attn.weight_orig', 'transformer.h.21.ln_1.weight_orig', 'transformer.h.37.ln_2.weight_orig', 'transformer.h.38.ln_2.weight_mask', 'transformer.h.7.attn.c_attn.weight_mask', 'transformer.h.38.mlp.c_fc.weight_orig', 'transformer.wte.weight_mask', 'transformer.h.32.ln_2.weight_orig', 'transformer.h.0.attn.c_proj.weight_mask', 'transformer.h.13.ln_2.weight_mask', 'transformer.h.36.ln_1.weight_orig', 'transformer.h.10.mlp.c_proj.weight_mask', 'transformer.h.10.attn.c_proj.weight_mask', 'transformer.h.14.attn.c_attn.weight_orig', 'transformer.h.37.mlp.c_fc.weight_mask', 'transformer.h.11.mlp.c_fc.weight_mask', 'transformer.h.40.ln_1.weight_orig', 'transformer.h.16.attn.c_attn.weight_mask', 'transformer.h.39.attn.c_attn.weight_mask', 'transformer.h.41.attn.c_attn.weight_mask', 'transformer.h.30.mlp.c_fc.weight_orig', 'transformer.h.41.attn.c_proj.weight_mask', 'transformer.h.20.mlp.c_fc.weight_mask', 'transformer.h.2.ln_2.weight_mask', 'transformer.h.45.attn.c_attn.weight_orig', 'transformer.h.28.attn.c_attn.weight_mask', 'transformer.h.12.attn.c_proj.weight_orig', 'transformer.h.39.attn.c_proj.weight_orig', 'transformer.h.47.attn.c_attn.weight_mask', 'transformer.h.4.ln_1.weight_mask', 'transformer.h.40.attn.c_attn.weight_orig', 'transformer.h.32.attn.c_proj.weight_mask', 'transformer.h.37.attn.c_attn.weight_mask', 'transformer.h.37.mlp.c_proj.weight_mask', 'transformer.h.3.attn.c_attn.weight_orig', 'transformer.h.24.mlp.c_fc.weight_mask', 'transformer.h.14.ln_2.weight_orig', 'transformer.h.31.ln_1.weight_orig', 'transformer.h.5.ln_2.weight_mask', 'transformer.h.44.mlp.c_proj.weight_mask', 'transformer.h.30.mlp.c_proj.weight_orig', 'transformer.h.22.ln_1.weight_orig', 'transformer.h.12.mlp.c_proj.weight_orig', 'transformer.h.26.mlp.c_proj.weight_mask', 'transformer.h.46.attn.c_proj.weight_mask', 'transformer.h.25.mlp.c_fc.weight_mask', 'transformer.h.8.mlp.c_fc.weight_mask', 'transformer.h.16.mlp.c_proj.weight_mask', 'transformer.h.1.mlp.c_fc.weight_orig', 'transformer.h.18.attn.c_proj.weight_mask', 'transformer.h.3.mlp.c_fc.weight_mask', 'transformer.h.20.mlp.c_proj.weight_orig', 'transformer.h.35.ln_2.weight_mask', 'transformer.h.41.ln_1.weight_mask', 'transformer.h.4.attn.c_attn.weight_orig', 'transformer.h.21.mlp.c_proj.weight_orig', 'transformer.h.42.ln_1.weight_mask', 'transformer.h.5.ln_1.weight_orig', 'transformer.h.15.mlp.c_fc.weight_orig', 'transformer.h.43.mlp.c_fc.weight_mask', 'transformer.h.18.ln_2.weight_mask', 'transformer.h.10.ln_2.weight_orig', 'lm_head.weight_mask', 'transformer.h.35.mlp.c_fc.weight_mask', 'transformer.h.23.mlp.c_proj.weight_mask', 'transformer.h.31.ln_2.weight_orig', 'transformer.h.47.mlp.c_fc.weight_orig', 'transformer.h.1.attn.c_attn.weight_orig', 'transformer.h.28.ln_2.weight_orig', 'transformer.h.47.ln_1.weight_orig', 'transformer.h.43.ln_2.weight_orig', 'transformer.h.46.ln_1.weight_orig', 'transformer.h.17.attn.c_attn.weight_mask', 'transformer.h.8.mlp.c_fc.weight_orig', 'transformer.h.11.mlp.c_fc.weight_orig', 'transformer.h.20.attn.c_attn.weight_mask', 'transformer.h.32.ln_1.weight_orig', 'transformer.h.23.ln_1.weight_orig', 'transformer.h.9.mlp.c_proj.weight_mask', 'transformer.h.18.mlp.c_fc.weight_mask', 'transformer.h.27.attn.c_attn.weight_mask', 'transformer.h.18.ln_2.weight_orig', 'transformer.h.34.attn.c_proj.weight_mask', 'transformer.h.13.mlp.c_proj.weight_orig', 'transformer.h.2.attn.c_attn.weight_orig', 'transformer.h.4.attn.c_attn.weight_mask', 'transformer.h.13.mlp.c_proj.weight_mask', 'transformer.h.8.ln_2.weight_orig', 'transformer.h.26.ln_2.weight_orig', 'transformer.h.42.ln_1.weight_orig', 'transformer.h.13.attn.c_proj.weight_mask', 'transformer.h.46.mlp.c_proj.weight_orig', 'transformer.h.45.attn.c_proj.weight_mask', 'transformer.h.42.attn.c_proj.weight_mask', 'transformer.h.1.ln_1.weight_mask', 'transformer.h.16.mlp.c_proj.weight_orig', 'transformer.h.33.attn.c_attn.weight_orig', 'transformer.h.9.mlp.c_fc.weight_orig', 'transformer.h.21.mlp.c_fc.weight_mask', 'transformer.h.22.mlp.c_fc.weight_mask', 'transformer.h.10.mlp.c_proj.weight_orig', 'transformer.h.44.ln_2.weight_orig', 'transformer.h.19.attn.c_proj.weight_mask', 'transformer.h.14.attn.c_proj.weight_mask', 'transformer.h.24.ln_2.weight_mask', 'transformer.h.25.mlp.c_proj.weight_mask', 'transformer.h.30.attn.c_proj.weight_mask', 'transformer.h.26.attn.c_proj.weight_orig', 'transformer.h.9.ln_2.weight_mask', 'transformer.h.22.ln_2.weight_mask', 'transformer.h.37.ln_1.weight_mask', 'transformer.h.11.attn.c_proj.weight_mask', 'transformer.h.2.ln_1.weight_mask', 'transformer.h.43.mlp.c_fc.weight_orig', 'transformer.h.47.mlp.c_fc.weight_mask', 'transformer.h.24.mlp.c_proj.weight_orig', 'transformer.h.19.ln_2.weight_mask', 'transformer.h.25.attn.c_proj.weight_mask', 'transformer.h.37.attn.c_attn.weight_orig', 'transformer.h.45.ln_1.weight_mask', 'transformer.h.27.mlp.c_proj.weight_orig', 'transformer.h.18.attn.c_attn.weight_mask', 'transformer.h.10.attn.c_attn.weight_orig', 'transformer.h.7.mlp.c_fc.weight_orig', 'transformer.h.18.mlp.c_fc.weight_orig', 'transformer.h.34.ln_1.weight_mask', 'transformer.h.24.attn.c_attn.weight_mask', 'transformer.h.1.attn.c_proj.weight_mask', 'transformer.h.40.attn.c_attn.weight_mask', 'transformer.h.19.attn.c_proj.weight_orig', 'transformer.h.33.ln_1.weight_orig', 'transformer.h.34.attn.c_attn.weight_orig', 'transformer.h.38.mlp.c_fc.weight_mask', 'transformer.h.3.attn.c_proj.weight_orig', 'transformer.h.2.attn.c_proj.weight_mask', 'transformer.h.12.ln_2.weight_orig', 'transformer.h.0.ln_2.weight_orig', 'transformer.h.22.attn.c_attn.weight_orig', 'transformer.h.25.attn.c_attn.weight_mask', 'transformer.h.29.mlp.c_proj.weight_orig', 'transformer.h.31.attn.c_attn.weight_orig', 'transformer.h.31.mlp.c_fc.weight_mask', 'transformer.h.31.attn.c_attn.weight_mask', 'transformer.h.31.mlp.c_proj.weight_orig', 'transformer.h.10.attn.c_attn.weight_mask', 'transformer.h.19.attn.c_attn.weight_mask', 'transformer.h.26.mlp.c_fc.weight_mask', 'transformer.h.39.mlp.c_fc.weight_mask', 'transformer.h.23.mlp.c_fc.weight_orig', 'transformer.h.46.ln_2.weight_orig', 'transformer.h.0.attn.c_attn.weight_mask', 'lm_head.weight_orig', 'transformer.h.12.attn.c_attn.weight_orig', 'transformer.h.5.ln_2.weight_orig', 'transformer.h.15.ln_2.weight_orig', 'transformer.h.11.attn.c_attn.weight_mask', 'transformer.h.27.ln_1.weight_mask', 'transformer.wpe.weight_mask', 'transformer.h.31.ln_1.weight_mask', 'transformer.h.0.ln_1.weight_orig', 'transformer.h.36.mlp.c_fc.weight_mask', 'transformer.h.8.mlp.c_proj.weight_orig', 'transformer.h.42.attn.c_proj.weight_orig', 'transformer.h.19.ln_2.weight_orig', 'transformer.h.24.ln_1.weight_mask', 'transformer.h.5.attn.c_attn.weight_mask', 'transformer.h.3.mlp.c_proj.weight_mask', 'transformer.h.34.mlp.c_fc.weight_orig', 'transformer.h.39.mlp.c_fc.weight_orig', 'transformer.h.18.mlp.c_proj.weight_orig', 'transformer.h.32.mlp.c_proj.weight_mask', 'transformer.h.41.mlp.c_proj.weight_mask', 'transformer.h.30.mlp.c_fc.weight_mask', 'transformer.h.4.mlp.c_fc.weight_orig', 'transformer.h.42.mlp.c_proj.weight_mask', 'transformer.h.15.mlp.c_proj.weight_mask', 'transformer.h.28.attn.c_proj.weight_orig', 'transformer.h.8.attn.c_proj.weight_orig', 'transformer.h.15.ln_2.weight_mask', 'transformer.h.14.mlp.c_fc.weight_mask', 'transformer.h.5.mlp.c_fc.weight_mask', 'transformer.h.17.mlp.c_fc.weight_mask', 'transformer.h.18.attn.c_proj.weight_orig', 'transformer.h.14.mlp.c_proj.weight_mask', 'transformer.h.7.ln_1.weight_mask', 'transformer.h.6.mlp.c_fc.weight_mask', 'transformer.h.39.ln_1.weight_orig', 'transformer.h.27.attn.c_attn.weight_orig', 'transformer.h.16.mlp.c_fc.weight_mask', 'transformer.h.35.attn.c_attn.weight_mask', 'transformer.h.38.attn.c_attn.weight_mask', 'transformer.h.23.ln_2.weight_mask', 'transformer.h.46.attn.c_proj.weight_orig', 'transformer.h.37.ln_1.weight_orig', 'transformer.h.40.ln_1.weight_mask', 'transformer.h.17.mlp.c_proj.weight_orig', 'transformer.h.1.ln_2.weight_mask', 'transformer.h.33.mlp.c_fc.weight_orig', 'transformer.h.43.attn.c_attn.weight_mask', 'transformer.h.29.attn.c_attn.weight_orig', 'transformer.h.45.ln_2.weight_mask', 'transformer.h.2.mlp.c_fc.weight_mask', 'transformer.h.29.ln_2.weight_mask', 'transformer.h.35.ln_1.weight_mask', 'transformer.h.11.ln_1.weight_orig', 'transformer.h.45.mlp.c_fc.weight_mask', 'transformer.h.5.attn.c_proj.weight_orig', 'transformer.h.23.ln_2.weight_orig', 'transformer.h.29.ln_2.weight_orig', 'transformer.h.1.ln_2.weight_orig', 'transformer.h.12.ln_1.weight_mask', 'transformer.h.13.mlp.c_fc.weight_orig', 'transformer.h.31.attn.c_proj.weight_orig', 'transformer.h.43.mlp.c_proj.weight_orig', 'transformer.h.7.mlp.c_fc.weight_mask', 'transformer.h.9.attn.c_proj.weight_mask', 'transformer.h.16.attn.c_attn.weight_orig', 'transformer.h.44.ln_1.weight_mask', 'transformer.h.30.attn.c_attn.weight_orig', 'transformer.h.4.ln_2.weight_orig', 'transformer.h.46.mlp.c_proj.weight_mask', 'transformer.h.32.ln_2.weight_mask', 'transformer.h.11.ln_2.weight_mask', 'transformer.h.2.ln_2.weight_orig', 'transformer.h.38.ln_1.weight_orig', 'transformer.h.35.ln_2.weight_orig', 'transformer.h.28.ln_2.weight_mask', 'transformer.h.0.ln_1.weight_mask', 'transformer.h.41.ln_2.weight_orig', 'transformer.h.29.mlp.c_proj.weight_mask', 'transformer.h.34.ln_1.weight_orig']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at ctkang/b_gpt2-xl_10 and are newly initialized: ['transformer.h.35.ln_2.weight', 'transformer.h.39.attn.c_attn.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.32.mlp.c_fc.weight', 'transformer.h.4.ln_1.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.19.mlp.c_fc.weight', 'transformer.h.46.ln_1.weight', 'transformer.h.30.attn.c_attn.weight', 'transformer.h.9.ln_2.weight', 'transformer.h.29.ln_2.weight', 'transformer.h.16.mlp.c_fc.weight', 'transformer.h.14.attn.c_attn.weight', 'transformer.h.15.ln_1.weight', 'transformer.h.41.ln_2.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.45.mlp.c_fc.weight', 'transformer.h.31.ln_2.weight', 'transformer.h.45.attn.c_proj.weight', 'transformer.h.39.mlp.c_proj.weight', 'transformer.h.29.mlp.c_fc.weight', 'transformer.h.27.attn.c_attn.weight', 'transformer.h.34.mlp.c_proj.weight', 'transformer.h.28.ln_1.weight', 'transformer.h.22.mlp.c_proj.weight', 'transformer.h.23.ln_1.weight', 'transformer.h.25.mlp.c_proj.weight', 'transformer.h.29.attn.c_proj.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.9.ln_1.weight', 'transformer.h.45.ln_1.weight', 'transformer.h.30.mlp.c_fc.weight', 'transformer.h.18.mlp.c_proj.weight', 'transformer.h.14.ln_2.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.12.mlp.c_fc.weight', 'transformer.h.41.attn.c_proj.weight', 'transformer.h.3.ln_2.weight', 'transformer.h.35.attn.c_attn.weight', 'transformer.h.20.ln_1.weight', 'transformer.h.34.ln_2.weight', 'transformer.h.20.attn.c_attn.weight', 'transformer.h.10.ln_2.weight', 'transformer.h.17.ln_2.weight', 'transformer.h.17.attn.c_attn.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.42.ln_1.weight', 'transformer.h.13.ln_1.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.25.attn.c_proj.weight', 'transformer.h.26.attn.c_proj.weight', 'transformer.h.43.attn.c_attn.weight', 'transformer.h.23.attn.c_proj.weight', 'transformer.h.15.mlp.c_fc.weight', 'transformer.h.17.mlp.c_fc.weight', 'transformer.h.22.ln_1.weight', 'transformer.h.39.mlp.c_fc.weight', 'transformer.h.40.mlp.c_fc.weight', 'transformer.h.12.mlp.c_proj.weight', 'transformer.h.22.attn.c_proj.weight', 'transformer.h.32.attn.c_attn.weight', 'transformer.h.18.attn.c_proj.weight', 'transformer.h.36.ln_1.weight', 'transformer.h.31.mlp.c_fc.weight', 'transformer.h.22.ln_2.weight', 'transformer.h.16.attn.c_attn.weight', 'transformer.h.38.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.40.ln_2.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.wpe.weight', 'transformer.h.36.mlp.c_proj.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.27.ln_2.weight', 'transformer.h.45.attn.c_attn.weight', 'transformer.h.37.mlp.c_fc.weight', 'transformer.h.41.ln_1.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.37.ln_2.weight', 'transformer.h.46.ln_2.weight', 'transformer.h.21.ln_2.weight', 'transformer.h.36.attn.c_proj.weight', 'transformer.h.19.attn.c_attn.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.20.mlp.c_proj.weight', 'transformer.h.10.ln_1.weight', 'transformer.wte.weight', 'transformer.h.47.ln_2.weight', 'transformer.h.27.mlp.c_fc.weight', 'transformer.h.43.mlp.c_proj.weight', 'transformer.h.28.mlp.c_fc.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.16.ln_1.weight', 'transformer.h.34.attn.c_proj.weight', 'transformer.h.31.attn.c_attn.weight', 'transformer.h.40.attn.c_attn.weight', 'transformer.h.12.ln_2.weight', 'transformer.h.3.ln_1.weight', 'transformer.h.5.ln_2.weight', 'transformer.h.37.ln_1.weight', 'transformer.h.43.ln_1.weight', 'transformer.h.21.attn.c_attn.weight', 'transformer.h.20.ln_2.weight', 'transformer.h.8.ln_1.weight', 'transformer.h.13.mlp.c_fc.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.43.ln_2.weight', 'transformer.h.11.ln_2.weight', 'transformer.h.16.attn.c_proj.weight', 'transformer.h.25.ln_2.weight', 'transformer.h.23.ln_2.weight', 'transformer.h.18.ln_2.weight', 'transformer.h.33.attn.c_proj.weight', 'transformer.h.44.ln_2.weight', 'transformer.h.35.mlp.c_fc.weight', 'transformer.h.30.ln_2.weight', 'transformer.h.42.attn.c_attn.weight', 'transformer.h.47.attn.c_attn.weight', 'transformer.h.47.attn.c_proj.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.28.attn.c_attn.weight', 'transformer.h.24.mlp.c_proj.weight', 'transformer.h.45.ln_2.weight', 'transformer.h.12.ln_1.weight', 'transformer.h.19.mlp.c_proj.weight', 'transformer.h.31.ln_1.weight', 'transformer.h.30.mlp.c_proj.weight', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.44.mlp.c_proj.weight', 'transformer.h.11.ln_1.weight', 'transformer.h.22.mlp.c_fc.weight', 'transformer.h.16.ln_2.weight', 'transformer.h.30.attn.c_proj.weight', 'transformer.h.33.ln_1.weight', 'transformer.h.39.attn.c_proj.weight', 'transformer.h.33.ln_2.weight', 'transformer.h.37.mlp.c_proj.weight', 'transformer.h.24.attn.c_attn.weight', 'transformer.h.38.mlp.c_fc.weight', 'transformer.h.24.mlp.c_fc.weight', 'transformer.h.26.attn.c_attn.weight', 'transformer.h.18.mlp.c_fc.weight', 'transformer.h.7.ln_2.weight', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.14.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.26.ln_1.weight', 'transformer.h.31.mlp.c_proj.weight', 'transformer.h.36.ln_2.weight', 'transformer.h.44.mlp.c_fc.weight', 'transformer.h.39.ln_2.weight', 'transformer.h.15.ln_2.weight', 'transformer.h.44.attn.c_attn.weight', 'transformer.h.38.mlp.c_proj.weight', 'transformer.h.18.ln_1.weight', 'transformer.h.22.attn.c_attn.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.17.ln_1.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.44.attn.c_proj.weight', 'transformer.h.33.mlp.c_fc.weight', 'transformer.h.46.attn.c_attn.weight', 'transformer.h.18.attn.c_attn.weight', 'transformer.h.27.mlp.c_proj.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.38.ln_1.weight', 'transformer.h.23.mlp.c_fc.weight', 'transformer.h.34.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.14.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.20.mlp.c_fc.weight', 'transformer.h.37.attn.c_proj.weight', 'transformer.h.43.mlp.c_fc.weight', 'transformer.h.28.ln_2.weight', 'transformer.h.35.attn.c_proj.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.36.attn.c_attn.weight', 'transformer.h.25.ln_1.weight', 'transformer.h.7.ln_1.weight', 'transformer.h.19.attn.c_proj.weight', 'transformer.h.4.ln_2.weight', 'transformer.h.13.ln_2.weight', 'transformer.h.27.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.40.mlp.c_proj.weight', 'transformer.h.34.ln_1.weight', 'transformer.h.19.ln_1.weight', 'transformer.h.19.ln_2.weight', 'transformer.h.20.attn.c_proj.weight', 'transformer.h.33.mlp.c_proj.weight', 'transformer.h.27.ln_1.weight', 'transformer.h.41.mlp.c_proj.weight', 'transformer.h.38.ln_2.weight', 'transformer.h.44.ln_1.weight', 'transformer.h.42.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.25.mlp.c_fc.weight', 'transformer.h.46.attn.c_proj.weight', 'transformer.h.46.mlp.c_proj.weight', 'transformer.h.34.mlp.c_fc.weight', 'transformer.h.17.attn.c_proj.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.1.ln_1.weight', 'transformer.h.25.attn.c_attn.weight', 'transformer.h.47.mlp.c_proj.weight', 'transformer.h.26.ln_2.weight', 'transformer.h.0.ln_2.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.40.ln_1.weight', 'transformer.h.12.attn.c_proj.weight', 'transformer.h.43.attn.c_proj.weight', 'transformer.h.21.attn.c_proj.weight', 'transformer.h.32.attn.c_proj.weight', 'transformer.h.47.mlp.c_fc.weight', 'transformer.h.21.mlp.c_fc.weight', 'transformer.h.42.ln_2.weight', 'transformer.h.41.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.45.mlp.c_proj.weight', 'transformer.h.42.attn.c_proj.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.24.ln_1.weight', 'transformer.h.2.ln_1.weight', 'transformer.h.35.ln_1.weight', 'transformer.ln_f.weight', 'transformer.h.14.ln_1.weight', 'transformer.h.46.mlp.c_fc.weight', 'transformer.h.12.attn.c_attn.weight', 'transformer.h.33.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.6.ln_2.weight', 'transformer.h.29.ln_1.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.26.mlp.c_proj.weight', 'transformer.h.13.attn.c_proj.weight', 'transformer.h.39.ln_1.weight', 'transformer.h.32.mlp.c_proj.weight', 'transformer.h.28.attn.c_proj.weight', 'transformer.h.28.mlp.c_proj.weight', 'transformer.h.23.attn.c_attn.weight', 'transformer.h.35.mlp.c_proj.weight', 'transformer.h.47.ln_1.weight', 'transformer.h.37.attn.c_attn.weight', 'transformer.h.29.attn.c_attn.weight', 'transformer.h.32.ln_2.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.15.attn.c_attn.weight', 'transformer.h.41.attn.c_attn.weight', 'transformer.h.40.attn.c_proj.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.21.ln_1.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.24.attn.c_proj.weight', 'transformer.h.17.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.31.attn.c_proj.weight', 'transformer.h.23.mlp.c_proj.weight', 'transformer.h.5.ln_1.weight', 'transformer.h.30.ln_1.weight', 'transformer.h.32.ln_1.weight', 'transformer.h.15.attn.c_proj.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.24.ln_2.weight', 'transformer.h.1.ln_2.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.42.mlp.c_fc.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.38.attn.c_proj.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.36.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.6.ln_1.weight', 'transformer.h.13.attn.c_attn.weight', 'transformer.h.8.ln_2.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.13.mlp.c_proj.weight', 'transformer.h.2.ln_2.weight', 'transformer.h.26.mlp.c_fc.weight', 'transformer.h.21.mlp.c_proj.weight', 'transformer.h.29.mlp.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ctkang/b_gpt2-xl_10\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ctkang/b_gpt2-xl_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 258/258 [00:00<00:00, 251kB/s]\n",
      "Downloading: 100%|██████████| 798k/798k [00:00<00:00, 7.99MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 5.95MB/s]\n",
      "Downloading: 100%|██████████| 2.11M/2.11M [00:00<00:00, 16.2MB/s]\n",
      "Downloading: 100%|██████████| 99.0/99.0 [00:00<00:00, 102kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ctkang/test_gpt_xl\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ctkang/test_gpt_xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/christopherkang/.conda/envs/llm/lib/python3.7/site-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'My name is Julien and I like to store integratedTalkfutureousy plasticabil skirtshesis scientSelf rupt skirts REG integrated hangingroximatelyNitIVERS BearsReally peasantsusk reuniteduskStock � hitterspe Draw diplomat hitters occasionallyifty neededhesis hanging Grahamprofessionalauntlet Null'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "generator(\n",
    "    \"My name is Julien and I like to\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune = [(m, \"weight\") for m in filter(lambda m: hasattr(m, \"weight\"), model.modules())]\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/christopherkang/.conda/envs/llm/lib/python3.7/site-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'My name is Julien and I like to make funny games. As an avid fan of all things comic book, I have a lot of ideas for various characters, story branches, etc., but that takes time and money away from making the games I'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "generator(\"My name is Julien and I like to\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(\"test_gpt_xl\")\n",
    "model.push_to_hub(\"test_gpt_xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ctkang/test_gpt_xl\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ctkang/test_gpt_xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "generator(\n",
    "    \"My name is Julien and I like to\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
